{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5af5592144442399aec923034319ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_98f592973a7a419c9c72976f0e1304ca"
          }
        },
        "61b84b2cc14445d38205ca77a818de06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dda35c05a1f4c1982f8ca2daefdb326",
            "placeholder": "​",
            "style": "IPY_MODEL_931413f48d63423d812db69e15061a60",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "96797bba0bd6444d9217436784172064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c6b2c10676184fdbb6707027dbb1f357",
            "placeholder": "​",
            "style": "IPY_MODEL_172e9b1bba62417fa4399e404ef1a79e",
            "value": ""
          }
        },
        "ae23890eca4841478c62c62f37343bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_cb8e25f1947944e8bf79b4cb872991e5",
            "style": "IPY_MODEL_66b5e5ac1f2e48de850ae5fdba21ebe2",
            "value": true
          }
        },
        "184da8a1930243869e2e7d46e50d5bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c381908420e94f56a37a09d4f431683a",
            "style": "IPY_MODEL_5b625463f45641ad9d3944695cec3571",
            "tooltip": ""
          }
        },
        "2f9c01d729e544e5b38af632cb65e1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22f9346a1b3d42cf84ae4e0cf360d7d6",
            "placeholder": "​",
            "style": "IPY_MODEL_f664061fa402464dbc3db0edba71e0ce",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "98f592973a7a419c9c72976f0e1304ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "4dda35c05a1f4c1982f8ca2daefdb326": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931413f48d63423d812db69e15061a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6b2c10676184fdbb6707027dbb1f357": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "172e9b1bba62417fa4399e404ef1a79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb8e25f1947944e8bf79b4cb872991e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b5e5ac1f2e48de850ae5fdba21ebe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c381908420e94f56a37a09d4f431683a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b625463f45641ad9d3944695cec3571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "22f9346a1b3d42cf84ae4e0cf360d7d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f664061fa402464dbc3db0edba71e0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28082382c36c4ef18e7dfb3c46f4e46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_059c0d5246914fcf92973ad29fe6bb48",
            "placeholder": "​",
            "style": "IPY_MODEL_7ce54d425f984691ab1aacd6eef6d198",
            "value": "Connecting..."
          }
        },
        "059c0d5246914fcf92973ad29fe6bb48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce54d425f984691ab1aacd6eef6d198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers wandb huggingface_hub trl pydantic"
      ],
      "metadata": {
        "id": "dO3bODU6YnjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "P-N49meJYyXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU unsloth"
      ],
      "metadata": {
        "id": "nnsXwelOu38g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "d5af5592144442399aec923034319ebf",
            "61b84b2cc14445d38205ca77a818de06",
            "96797bba0bd6444d9217436784172064",
            "ae23890eca4841478c62c62f37343bb0",
            "184da8a1930243869e2e7d46e50d5bb5",
            "2f9c01d729e544e5b38af632cb65e1c6",
            "98f592973a7a419c9c72976f0e1304ca",
            "4dda35c05a1f4c1982f8ca2daefdb326",
            "931413f48d63423d812db69e15061a60",
            "c6b2c10676184fdbb6707027dbb1f357",
            "172e9b1bba62417fa4399e404ef1a79e",
            "cb8e25f1947944e8bf79b4cb872991e5",
            "66b5e5ac1f2e48de850ae5fdba21ebe2",
            "c381908420e94f56a37a09d4f431683a",
            "5b625463f45641ad9d3944695cec3571",
            "22f9346a1b3d42cf84ae4e0cf360d7d6",
            "f664061fa402464dbc3db0edba71e0ce",
            "28082382c36c4ef18e7dfb3c46f4e46f",
            "059c0d5246914fcf92973ad29fe6bb48",
            "7ce54d425f984691ab1aacd6eef6d198"
          ]
        },
        "id": "KehpI75Vdom2",
        "outputId": "9052451f-2da6-430a-9ec9-9c9c0182e0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5af5592144442399aec923034319ebf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvvQNbGYYUgh"
      },
      "outputs": [],
      "source": [
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, Any, Optional, List\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import Dict, Any, Optional, List\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from huggingface_hub import HfApi, create_repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Configuration Management\n",
        "# ================================\n",
        "class ModelConfig(BaseModel):\n",
        "    \"\"\"Model configuration settings\"\"\"\n",
        "    base_model: str = Field(default=\"unsloth/llama-3-8b-bnb-4bit\", description=\"Base model to fine-tune\")\n",
        "    max_seq_length: int = Field(default=2048, gt=0, description=\"Maximum sequence length\")\n",
        "    dtype: Optional[str] = Field(default=None, description=\"Data type for model weights\")\n",
        "    load_in_4bit: bool = Field(default=True, description=\"Load model in 4-bit quantization\")\n",
        "\n",
        "    @field_validator('max_seq_length')\n",
        "    @classmethod\n",
        "    def validate_seq_length(cls, v):\n",
        "        if v <= 0:\n",
        "            raise ValueError('max_seq_length must be positive')\n",
        "        return v\n",
        "\n",
        "class LoRAConfig(BaseModel):\n",
        "    \"\"\"LoRA configuration for fine-tuning\"\"\"\n",
        "    r: int = Field(default=16, gt=0, description=\"LoRA rank\")\n",
        "    target_modules: List[str] = Field(\n",
        "        default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        description=\"Target modules for LoRA\"\n",
        "    )\n",
        "    lora_alpha: int = Field(default=16, gt=0, description=\"LoRA alpha parameter\")\n",
        "    lora_dropout: float = Field(default=0.0, ge=0.0, le=1.0, description=\"LoRA dropout rate\")\n",
        "    bias: str = Field(default=\"none\", description=\"Bias type for LoRA\")\n",
        "    use_gradient_checkpointing: str = Field(default=\"unsloth\", description=\"Gradient checkpointing method\")\n",
        "    random_state: int = Field(default=3407, description=\"Random seed\")\n",
        "    use_rslora: bool = Field(default=False, description=\"Use RSLoRA\")\n",
        "    loftq_config: Optional[Dict] = Field(default=None, description=\"LoftQ configuration\")\n",
        "\n",
        "    @field_validator('bias')\n",
        "    @classmethod\n",
        "    def validate_bias(cls, v):\n",
        "        allowed_bias = [\"none\", \"all\", \"lora_only\"]\n",
        "        if v not in allowed_bias:\n",
        "            raise ValueError(f'bias must be one of {allowed_bias}')\n",
        "        return v\n",
        "\n",
        "class SFTConfig(BaseModel):\n",
        "    \"\"\"Supervised Fine-tuning configuration\"\"\"\n",
        "    output_dir: str = Field(default=\"./sft_results\", description=\"Output directory for SFT\")\n",
        "    per_device_train_batch_size: int = Field(default=2, gt=0, description=\"Training batch size per device\")\n",
        "    gradient_accumulation_steps: int = Field(default=4, gt=0, description=\"Gradient accumulation steps\")\n",
        "    warmup_steps: int = Field(default=5, ge=0, description=\"Warmup steps\")\n",
        "    max_steps: int = Field(default=100, gt=0, description=\"Maximum training steps\")\n",
        "    learning_rate: float = Field(default=2e-4, gt=0, description=\"Learning rate\")\n",
        "    fp16: bool = Field(default=True, description=\"Use FP16 precision\")\n",
        "    bf16: bool = Field(default=False, description=\"Use BF16 precision\")\n",
        "    logging_steps: int = Field(default=1, gt=0, description=\"Logging frequency\")\n",
        "    optim: str = Field(default=\"adamw_8bit\", description=\"Optimizer\")\n",
        "    weight_decay: float = Field(default=0.01, ge=0, description=\"Weight decay\")\n",
        "    lr_scheduler_type: str = Field(default=\"linear\", description=\"Learning rate scheduler\")\n",
        "    seed: int = Field(default=3407, description=\"Random seed\")\n",
        "    dataset_text_field: str = Field(default=\"text\", description=\"Dataset text field name\")\n",
        "\n",
        "    @field_validator('optim')\n",
        "    @classmethod\n",
        "    def validate_optimizer(cls, v):\n",
        "        allowed_optims = [\"adamw_8bit\", \"adamw\", \"sgd\", \"adafactor\"]\n",
        "        if v not in allowed_optims:\n",
        "            raise ValueError(f'optim must be one of {allowed_optims}')\n",
        "        return v\n",
        "\n",
        "class DPOConfig(BaseModel):\n",
        "    \"\"\"Direct Preference Optimization configuration\"\"\"\n",
        "    output_dir: str = Field(default=\"./dpo_results\", description=\"Output directory for DPO\")\n",
        "    per_device_train_batch_size: int = Field(default=1, gt=0, description=\"Training batch size per device\")\n",
        "    gradient_accumulation_steps: int = Field(default=8, gt=0, description=\"Gradient accumulation steps\")\n",
        "    warmup_steps: int = Field(default=5, ge=0, description=\"Warmup steps\")\n",
        "    max_steps: int = Field(default=50, gt=0, description=\"Maximum training steps\")\n",
        "    learning_rate: float = Field(default=5e-7, gt=0, description=\"Learning rate\")\n",
        "    fp16: bool = Field(default=True, description=\"Use FP16 precision\")\n",
        "    bf16: bool = Field(default=False, description=\"Use BF16 precision\")\n",
        "    logging_steps: int = Field(default=1, gt=0, description=\"Logging frequency\")\n",
        "    optim: str = Field(default=\"adamw_8bit\", description=\"Optimizer\")\n",
        "    weight_decay: float = Field(default=0.01, ge=0, description=\"Weight decay\")\n",
        "    lr_scheduler_type: str = Field(default=\"linear\", description=\"Learning rate scheduler\")\n",
        "    seed: int = Field(default=3407, description=\"Random seed\")\n",
        "    beta: float = Field(default=0.1, gt=0, description=\"DPO beta parameter\")\n",
        "    dataset_num_proc: int = Field(default=4, gt=0, description=\"Number of processes for dataset\")\n",
        "\n",
        "    @field_validator('beta')\n",
        "    @classmethod\n",
        "    def validate_beta(cls, v):\n",
        "        if v <= 0:\n",
        "            raise ValueError('beta must be positive')\n",
        "        return v\n",
        "\n",
        "class HubConfig(BaseModel):\n",
        "    \"\"\"HuggingFace Hub configuration\"\"\"\n",
        "    repo_id: str = Field(default=\"rahulsamant37/voice-assistant-model\", description=\"HuggingFace repository ID\")\n",
        "    private: bool = Field(default=False, description=\"Make repository private\")\n",
        "    token: Optional[str] = Field(default=None, description=\"HuggingFace API token\")\n",
        "    commit_message: str = Field(default=\"Upload fine-tuned voice model\", description=\"Commit message\")\n",
        "\n",
        "    @field_validator('repo_id')\n",
        "    @classmethod\n",
        "    def validate_repo_id(cls, v):\n",
        "        if '/' not in v:\n",
        "            raise ValueError('repo_id must be in format \"username/repo-name\"')\n",
        "        return v\n",
        "\n",
        "class PipelineConfig(BaseModel):\n",
        "    \"\"\"Main pipeline configuration\"\"\"\n",
        "    model: ModelConfig = Field(default_factory=ModelConfig, description=\"Model configuration\")\n",
        "    lora: LoRAConfig = Field(default_factory=LoRAConfig, description=\"LoRA configuration\")\n",
        "    sft: SFTConfig = Field(default_factory=SFTConfig, description=\"SFT configuration\")\n",
        "    dpo: DPOConfig = Field(default_factory=DPOConfig, description=\"DPO configuration\")\n",
        "    hub: HubConfig = Field(default_factory=HubConfig, description=\"Hub configuration\")\n",
        "    wandb_project: str = Field(default=\"voice-finetuning\", description=\"Weights & Biases project name\")\n",
        "    use_wandb: bool = Field(default=False, description=\"Enable Weights & Biases logging\") # Changed default to False\n",
        "    save_intermediate: bool = Field(default=True, description=\"Save intermediate checkpoints\")\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Pydantic configuration\"\"\"\n",
        "        validate_assignment = True\n",
        "        extra = \"forbid\"\n",
        "        use_enum_values = True"
      ],
      "metadata": {
        "id": "5GQ6kQRDYvPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Logging Service\n",
        "# ================================\n",
        "class LoggingService:\n",
        "    \"\"\"Centralized logging service\"\"\"\n",
        "\n",
        "    def __init__(self, name: str = \"voice_pipeline\", level: str = \"INFO\"):\n",
        "        self.logger = logging.getLogger(name)\n",
        "        self.logger.setLevel(getattr(logging, level.upper()))\n",
        "\n",
        "        if not self.logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter(\n",
        "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "            )\n",
        "            handler.setFormatter(formatter)\n",
        "            self.logger.addHandler(handler)\n",
        "\n",
        "    def info(self, message: str):\n",
        "        self.logger.info(message)\n",
        "\n",
        "    def error(self, message: str):\n",
        "        self.logger.error(message)\n",
        "\n",
        "    def warning(self, message: str):\n",
        "        self.logger.warning(message)"
      ],
      "metadata": {
        "id": "7iut7wzQY7MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Data Service\n",
        "# ================================\n",
        "class DataService:\n",
        "    \"\"\"Service for handling dataset operations\"\"\"\n",
        "\n",
        "    def __init__(self, logger: LoggingService):\n",
        "        self.logger = logger\n",
        "\n",
        "    def load_sft_dataset(self) -> Dataset:\n",
        "        \"\"\"Load dataset for supervised fine-tuning - UPDATED with working datasets\"\"\"\n",
        "        self.logger.info(\"Loading SFT dataset...\")\n",
        "\n",
        "        try:\n",
        "            # Try multiple datasets until one works\n",
        "            datasets_to_try = [\n",
        "                (\"lmsys/chatbot_arena_conversations\", \"train[:1000]\"),\n",
        "                (\"HuggingFaceH4/ultrachat_200k\", \"train_sft[:1000]\"),\n",
        "                (\"microsoft/DialoGPT-medium\", None),  # Keep as fallback\n",
        "                (\"Anthropic/hh-rlhf\", \"train[:1000]\"),\n",
        "            ]\n",
        "\n",
        "            dataset = None\n",
        "            for dataset_name, split in datasets_to_try:\n",
        "                try:\n",
        "                    self.logger.info(f\"Trying dataset: {dataset_name}\")\n",
        "                    if split:\n",
        "                        dataset = load_dataset(dataset_name, split=split)\n",
        "                    else:\n",
        "                        dataset = load_dataset(dataset_name, split=\"train[:1000]\")\n",
        "                    self.logger.info(f\"Successfully loaded: {dataset_name}\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    self.logger.warning(f\"Failed to load {dataset_name}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if dataset is None:\n",
        "                # Create a synthetic dataset as last resort\n",
        "                self.logger.info(\"Creating synthetic conversation dataset...\")\n",
        "                dataset = self._create_synthetic_dataset()\n",
        "\n",
        "            # Format the dataset for conversation\n",
        "            formatted_dataset = self._format_sft_dataset(dataset)\n",
        "            self.logger.info(f\"Loaded {len(formatted_dataset)} SFT examples\")\n",
        "            return formatted_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"All dataset loading attempts failed: {str(e)}\")\n",
        "            # Create synthetic dataset as final fallback\n",
        "            return self._create_synthetic_dataset()\n",
        "\n",
        "    def _create_synthetic_dataset(self) -> Dataset:\n",
        "        \"\"\"Create a synthetic conversational dataset\"\"\"\n",
        "        self.logger.info(\"Creating synthetic conversational dataset...\")\n",
        "\n",
        "        # Sample conversation data for voice assistant training\n",
        "        conversations = [\n",
        "            {\"text\": \"<|im_start|>user\\nWhat's the weather like today?<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to help you with the weather, but I don't have access to real-time weather data. Could you please tell me your location so I can provide more specific guidance?<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nSet a timer for 10 minutes<|im_end|>\\n<|im_start|>assistant\\nI'll set a timer for 10 minutes. The timer is now running and will alert you when the time is up.<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nPlay some music<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to help you play music! However, I don't have direct access to music streaming services. You can try saying 'Hey Google, play music' or 'Alexa, play music' to your smart speaker, or open your favorite music app.<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nWhat time is it?<|im_end|>\\n<|im_start|>assistant\\nI don't have access to real-time information, but you can check the time on your device or ask your voice assistant. Is there anything else I can help you with?<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nTell me a joke<|im_end|>\\n<|im_start|>assistant\\nHere's a joke for you: Why don't scientists trust atoms? Because they make up everything! I hope that brought a smile to your face.<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nWhat can you help me with?<|im_end|>\\n<|im_start|>assistant\\nI'm a voice assistant and I can help you with various tasks like answering questions, providing information, helping with calculations, setting reminders, and having conversations. What would you like assistance with today?<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nCall my mom<|im_end|>\\n<|im_start|>assistant\\nI'm not able to make phone calls directly, but I can guide you on how to call your mom. You can say 'Call Mom' to your phone's voice assistant, or find her contact in your phone app and tap to call.<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nWhat's 25 plus 37?<|im_end|>\\n<|im_start|>assistant\\n25 plus 37 equals 62.<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nTurn off the lights<|im_end|>\\n<|im_start|>assistant\\nI can't directly control your lights, but if you have smart lights, you can try saying 'Turn off the lights' to your smart home assistant, or use your smart home app to control them.<|im_end|>\"},\n",
        "            {\"text\": \"<|im_start|>user\\nGood morning<|im_end|>\\n<|im_start|>assistant\\nGood morning! I hope you're having a wonderful start to your day. How can I assist you this morning?<|im_end|>\"},\n",
        "        ]\n",
        "\n",
        "        # Expand the dataset by creating variations\n",
        "        expanded_conversations = []\n",
        "        for conv in conversations:\n",
        "            expanded_conversations.append(conv)\n",
        "\n",
        "        # Add more variations to reach 100+ examples\n",
        "        for i in range(10):\n",
        "            for conv in conversations:\n",
        "                expanded_conversations.append(conv)\n",
        "\n",
        "        dataset = Dataset.from_list(expanded_conversations)\n",
        "        self.logger.info(f\"Created synthetic dataset with {len(dataset)} examples\")\n",
        "        return dataset\n",
        "\n",
        "    def _format_sft_dataset(self, dataset: Dataset) -> Dataset:\n",
        "        \"\"\"Format dataset for SFT training\"\"\"\n",
        "        def format_conversation(examples):\n",
        "            formatted_texts = []\n",
        "\n",
        "            # Handle different dataset formats\n",
        "            if 'text' in examples:\n",
        "                # Already formatted\n",
        "                formatted_texts = examples['text']\n",
        "            elif 'conversation' in examples:\n",
        "                # Chatbot arena format\n",
        "                for conv in examples['conversation']:\n",
        "                    if isinstance(conv, list) and len(conv) >= 2:\n",
        "                        formatted_text = f\"<|im_start|>user\\n{conv[0]['content']}<|im_end|>\\n<|im_start|>assistant\\n{conv[1]['content']}<|im_end|>\"\n",
        "                        formatted_texts.append(formatted_text)\n",
        "            elif 'chosen' in examples:\n",
        "                # HH-RLHF format\n",
        "                for chosen in examples['chosen']:\n",
        "                    if 'Human:' in chosen and 'Assistant:' in chosen:\n",
        "                        parts = chosen.split('Assistant:')\n",
        "                        if len(parts) >= 2:\n",
        "                            human_part = parts[0].replace('Human:', '').strip()\n",
        "                            assistant_part = parts[1].strip()\n",
        "                            formatted_text = f\"<|im_start|>user\\n{human_part}<|im_end|>\\n<|im_start|>assistant\\n{assistant_part}<|im_end|>\"\n",
        "                            formatted_texts.append(formatted_text)\n",
        "            else:\n",
        "                # Generic fallback - use first text field available\n",
        "                text_fields = [k for k in examples.keys() if isinstance(examples[k][0], str)]\n",
        "                if text_fields:\n",
        "                    for text in examples[text_fields[0]]:\n",
        "                        formatted_text = f\"<|im_start|>user\\n{text}<|im_end|>\\n<|im_start|>assistant\\n{text}<|im_end|>\"\n",
        "                        formatted_texts.append(formatted_text)\n",
        "\n",
        "            return {\"text\": formatted_texts}\n",
        "\n",
        "        try:\n",
        "            formatted_dataset = dataset.map(format_conversation, batched=True, remove_columns=dataset.column_names)\n",
        "            return formatted_dataset\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error formatting dataset: {str(e)}, using as-is\")\n",
        "            # If formatting fails, ensure we have a 'text' column\n",
        "            if 'text' not in dataset.column_names:\n",
        "                first_text_col = [col for col in dataset.column_names if dataset[col][0] and isinstance(dataset[col][0], str)]\n",
        "                if first_text_col:\n",
        "                    dataset = dataset.rename_column(first_text_col[0], 'text')\n",
        "            return dataset\n",
        "\n",
        "    def load_dpo_dataset(self) -> Dataset:\n",
        "        \"\"\"Load dataset for DPO training - UPDATED\"\"\"\n",
        "        self.logger.info(\"Loading DPO dataset...\")\n",
        "\n",
        "        try:\n",
        "            # Try Anthropic HH-RLHF first\n",
        "            try:\n",
        "                dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:500]\")\n",
        "                self.logger.info(\"Successfully loaded Anthropic/hh-rlhf dataset\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Failed to load Anthropic/hh-rlhf: {str(e)}\")\n",
        "                # Create synthetic DPO dataset\n",
        "                dataset = self._create_synthetic_dpo_dataset()\n",
        "\n",
        "            def format_dpo_data(examples):\n",
        "                prompts = []\n",
        "                chosen = []\n",
        "                rejected = []\n",
        "\n",
        "                if 'chosen' in examples and 'rejected' in examples:\n",
        "                    # Standard DPO format\n",
        "                    for i in range(len(examples[\"chosen\"])):\n",
        "                        try:\n",
        "                            chosen_text = examples[\"chosen\"][i]\n",
        "                            rejected_text = examples[\"rejected\"][i]\n",
        "\n",
        "                            if \"Assistant:\" in chosen_text:\n",
        "                                prompt = chosen_text.split(\"Assistant:\")[0] + \"Assistant:\"\n",
        "                                chosen_response = chosen_text.split(\"Assistant:\")[-1].strip()\n",
        "                                rejected_response = rejected_text.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "                                prompts.append(prompt)\n",
        "                                chosen.append(chosen_response)\n",
        "                                rejected.append(rejected_response)\n",
        "                        except (IndexError, AttributeError):\n",
        "                            continue\n",
        "                else:\n",
        "                    # Use synthetic data\n",
        "                    return self._get_synthetic_dpo_batch()\n",
        "\n",
        "                return {\n",
        "                    \"prompt\": prompts,\n",
        "                    \"chosen\": chosen,\n",
        "                    \"rejected\": rejected\n",
        "                }\n",
        "\n",
        "            formatted_dataset = dataset.map(format_dpo_data, batched=True)\n",
        "            # Filter out empty examples\n",
        "            formatted_dataset = formatted_dataset.filter(lambda x: len(x[\"prompt\"]) > 0)\n",
        "\n",
        "            self.logger.info(f\"Loaded {len(formatted_dataset)} DPO examples\")\n",
        "            return formatted_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load DPO dataset: {str(e)}\")\n",
        "            return self._create_synthetic_dpo_dataset()\n",
        "\n",
        "    def _create_synthetic_dpo_dataset(self) -> Dataset:\n",
        "        \"\"\"Create synthetic DPO dataset\"\"\"\n",
        "        self.logger.info(\"Creating synthetic DPO dataset...\")\n",
        "\n",
        "        dpo_examples = [\n",
        "            {\n",
        "                \"prompt\": \"Human: What's the weather like today? Assistant:\",\n",
        "                \"chosen\": \"I'd be happy to help you with the weather, but I don't have access to real-time weather data. Could you please tell me your location?\",\n",
        "                \"rejected\": \"It's sunny and warm everywhere today!\"\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Human: Tell me a joke Assistant:\",\n",
        "                \"chosen\": \"Here's a joke for you: Why don't scientists trust atoms? Because they make up everything!\",\n",
        "                \"rejected\": \"Jokes are stupid and waste of time.\"\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Human: How do I cook pasta? Assistant:\",\n",
        "                \"chosen\": \"To cook pasta: 1) Boil water in a large pot, 2) Add salt, 3) Add pasta and cook according to package directions, 4) Drain and serve!\",\n",
        "                \"rejected\": \"Just put pasta in cold water and hope for the best.\"\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Human: What's 2+2? Assistant:\",\n",
        "                \"chosen\": \"2 + 2 equals 4.\",\n",
        "                \"rejected\": \"2 + 2 equals 5, obviously.\"\n",
        "            },\n",
        "            {\n",
        "                \"prompt\": \"Human: Help me with my homework Assistant:\",\n",
        "                \"chosen\": \"I'd be happy to help you with your homework! What subject are you working on and what specific questions do you have?\",\n",
        "                \"rejected\": \"I'm not going to do your homework for you. Figure it out yourself.\"\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Expand the dataset\n",
        "        expanded_examples = []\n",
        "        for _ in range(20):  # Create 100 examples\n",
        "            for example in dpo_examples:\n",
        "                expanded_examples.append(example)\n",
        "\n",
        "        dataset = Dataset.from_list(expanded_examples)\n",
        "        self.logger.info(f\"Created synthetic DPO dataset with {len(dataset)} examples\")\n",
        "        return dataset\n",
        "\n",
        "    def _get_synthetic_dpo_batch(self):\n",
        "        \"\"\"Get a batch of synthetic DPO data\"\"\"\n",
        "        return {\n",
        "            \"prompt\": [\"Human: Hello Assistant:\", \"Human: What time is it? Assistant:\"],\n",
        "            \"chosen\": [\"Hello! How can I assist you today?\", \"I don't have access to real-time information, but you can check your device for the current time.\"],\n",
        "            \"rejected\": [\"Go away.\", \"Time is an illusion.\"]\n",
        "        }"
      ],
      "metadata": {
        "id": "2nZ7K7zuY9bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Model Service\n",
        "# ================================\n",
        "class ModelService:\n",
        "    \"\"\"Service for model operations\"\"\"\n",
        "\n",
        "    def __init__(self, config: PipelineConfig, logger: LoggingService):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_base_model(self):\n",
        "        \"\"\"Load the base model and tokenizer\"\"\"\n",
        "        self.logger.info(f\"Loading base model: {self.config.model.base_model}\")\n",
        "\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.config.model.base_model,\n",
        "            max_seq_length=self.config.model.max_seq_length,\n",
        "            dtype=self.config.model.dtype,\n",
        "            load_in_4bit=self.config.model.load_in_4bit,\n",
        "            # Added parameters for offloading/device map\n",
        "            llm_int8_enable_fp32_cpu_offload=True,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "        self.logger.info(\"Base model loaded successfully\")\n",
        "\n",
        "    def prepare_for_training(self):\n",
        "        \"\"\"Prepare model for training with LoRA\"\"\"\n",
        "        self.logger.info(\"Preparing model for training...\")\n",
        "\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=self.config.lora.r,\n",
        "            target_modules=self.config.lora.target_modules,\n",
        "            lora_alpha=self.config.lora.lora_alpha,\n",
        "            lora_dropout=self.config.lora.lora_dropout,\n",
        "            bias=self.config.lora.bias,\n",
        "            use_gradient_checkpointing=self.config.lora.use_gradient_checkpointing,\n",
        "            random_state=self.config.lora.random_state,\n",
        "            use_rslora=self.config.lora.use_rslora,\n",
        "            loftq_config=self.config.lora.loftq_config,\n",
        "        )\n",
        "\n",
        "        self.logger.info(\"Model prepared for training\")"
      ],
      "metadata": {
        "id": "rEK8NpKmZAt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Training Services\n",
        "# ================================\n",
        "\n",
        "class SFTService:\n",
        "    \"\"\"Supervised Fine-tuning service\"\"\"\n",
        "\n",
        "    def __init__(self, config: PipelineConfig, logger: LoggingService):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def train(self, model, tokenizer, dataset: Dataset):\n",
        "        \"\"\"Run supervised fine-tuning\"\"\"\n",
        "        self.logger.info(\"Starting SFT training...\")\n",
        "\n",
        "        from trl import SFTTrainer\n",
        "\n",
        "        trainer = SFTTrainer(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            train_dataset=dataset,\n",
        "            dataset_text_field=self.config.sft.dataset_text_field,\n",
        "            max_seq_length=self.config.model.max_seq_length,\n",
        "            dataset_num_proc=2,\n",
        "            packing=False,\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=self.config.sft.per_device_train_batch_size,\n",
        "                gradient_accumulation_steps=self.config.sft.gradient_accumulation_steps,\n",
        "                warmup_steps=self.config.sft.warmup_steps,\n",
        "                max_steps=self.config.sft.max_steps,\n",
        "                learning_rate=self.config.sft.learning_rate,\n",
        "                fp16=self.config.sft.fp16,\n",
        "                bf16=self.config.sft.bf16,\n",
        "                logging_steps=self.config.sft.logging_steps,\n",
        "                optim=self.config.sft.optim,\n",
        "                weight_decay=self.config.sft.weight_decay,\n",
        "                lr_scheduler_type=self.config.sft.lr_scheduler_type,\n",
        "                seed=self.config.sft.seed,\n",
        "                output_dir=self.config.sft.output_dir,\n",
        "                report_to=\"none\", # Changed report_to to \"none\"\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        self.logger.info(\"SFT training completed\")\n",
        "\n",
        "        if self.config.save_intermediate:\n",
        "            trainer.save_model()\n",
        "            self.logger.info(f\"SFT model saved to {self.config.sft.output_dir}\")\n",
        "\n",
        "        return trainer"
      ],
      "metadata": {
        "id": "_TY1hzUgqiEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DPOService:\n",
        "    \"\"\"Direct Preference Optimization service\"\"\"\n",
        "\n",
        "    def __init__(self, config: PipelineConfig, logger: LoggingService):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "    def train(self, model, tokenizer, dataset: Dataset):\n",
        "        \"\"\"Run DPO training\"\"\"\n",
        "        self.logger.info(\"Starting DPO training...\")\n",
        "\n",
        "        from trl import DPOTrainer\n",
        "\n",
        "        trainer = DPOTrainer(\n",
        "            model=model,\n",
        "            ref_model=None,  # Use implicit reference model\n",
        "            args=TrainingArguments(\n",
        "                per_device_train_batch_size=self.config.dpo.per_device_train_batch_size,\n",
        "                gradient_accumulation_steps=self.config.dpo.gradient_accumulation_steps,\n",
        "                warmup_steps=self.config.dpo.warmup_steps,\n",
        "                max_steps=self.config.dpo.max_steps,\n",
        "                learning_rate=self.config.dpo.learning_rate,\n",
        "                fp16=self.config.dpo.fp16,\n",
        "                bf16=self.config.dpo.bf16,\n",
        "                logging_steps=self.config.dpo.logging_steps,\n",
        "                optim=self.config.dpo.optim,\n",
        "                weight_decay=self.config.dpo.weight_decay,\n",
        "                lr_scheduler_type=self.config.dpo.lr_scheduler_type,\n",
        "                seed=self.config.dpo.seed,\n",
        "                output_dir=self.config.dpo.output_dir,\n",
        "                remove_unused_columns=False,\n",
        "                report_to=\"none\", # Changed report_to to \"none\"\n",
        "            ),\n",
        "            beta=self.config.dpo.beta,\n",
        "            train_dataset=dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            max_length=self.config.model.max_seq_length,\n",
        "            max_prompt_length=self.config.model.max_seq_length // 2,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        self.logger.info(\"DPO training completed\")\n",
        "\n",
        "        trainer.save_model()\n",
        "        self.logger.info(f\"DPO model saved to {self.config.dpo.output_dir}\")\n",
        "\n",
        "        return trainer"
      ],
      "metadata": {
        "id": "iahK0t5XZGKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Hub Service\n",
        "# ================================\n",
        "class HubService:\n",
        "    \"\"\"HuggingFace Hub service\"\"\"\n",
        "\n",
        "    def __init__(self, config: PipelineConfig, logger: LoggingService):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.api = HfApi(token=self.config.hub.token)\n",
        "\n",
        "    def push_to_hub(self, model_path: str):\n",
        "        \"\"\"Push model to HuggingFace Hub\"\"\"\n",
        "        self.logger.info(f\"Pushing model to Hub: {self.config.hub.repo_id}\")\n",
        "\n",
        "        try:\n",
        "            # Create repository if it doesn't exist\n",
        "            create_repo(\n",
        "                repo_id=self.config.hub.repo_id,\n",
        "                private=self.config.hub.private,\n",
        "                token=self.config.hub.token,\n",
        "                exist_ok=True\n",
        "            )\n",
        "\n",
        "            # Upload model files\n",
        "            self.api.upload_folder(\n",
        "                folder_path=model_path,\n",
        "                repo_id=self.config.hub.repo_id,\n",
        "                commit_message=self.config.hub.commit_message,\n",
        "                token=self.config.hub.token\n",
        "            )\n",
        "\n",
        "            self.logger.info(\"Model successfully pushed to Hub\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to push to Hub: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "OwQQHot6ZIOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Main Pipeline Orchestrator\n",
        "# ================================\n",
        "\n",
        "class VoiceFinetuningPipeline:\n",
        "    \"\"\"Main pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self, config_path: Optional[str] = None):\n",
        "        # Load configuration\n",
        "        if config_path and os.path.exists(config_path):\n",
        "            with open(config_path, 'r') as f:\n",
        "                config_dict = json.load(f)\n",
        "                self.config = PipelineConfig(**config_dict)\n",
        "        else:\n",
        "            self.config = PipelineConfig()\n",
        "\n",
        "        # Initialize services\n",
        "        self.logger = LoggingService()\n",
        "        self.data_service = DataService(self.logger)\n",
        "        self.model_service = ModelService(self.config, self.logger)\n",
        "        self.sft_service = SFTService(self.config, self.logger)\n",
        "        self.dpo_service = DPOService(self.config, self.logger)\n",
        "        self.hub_service = HubService(self.config, self.logger)\n",
        "\n",
        "    def save_config(self, path: str = \"pipeline_config.json\"):\n",
        "        \"\"\"Save current configuration to file\"\"\"\n",
        "        from dataclasses import asdict # Import asdict\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(asdict(self.config), f, indent=2, default=str)\n",
        "        self.logger.info(f\"Configuration saved to {path}\")\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Run the complete fine-tuning pipeline\"\"\"\n",
        "        self.logger.info(\"Starting Voice Fine-tuning Pipeline\")\n",
        "\n",
        "        # Initialize W&B if enabled\n",
        "        # Removed wandb.init()\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load base model\n",
        "            self.model_service.load_base_model()\n",
        "            self.model_service.prepare_for_training()\n",
        "\n",
        "            # Step 2: SFT Training\n",
        "            self.logger.info(\"=\" * 50)\n",
        "            self.logger.info(\"PHASE 1: Supervised Fine-tuning\")\n",
        "            self.logger.info(\"=\" * 50)\n",
        "\n",
        "            sft_dataset = self.data_service.load_sft_dataset()\n",
        "            sft_trainer = self.sft_service.train(\n",
        "                self.model_service.model,\n",
        "                self.model_service.tokenizer,\n",
        "                sft_dataset\n",
        "            )\n",
        "\n",
        "            # Step 3: DPO Training\n",
        "            self.logger.info(\"=\" * 50)\n",
        "            self.logger.info(\"PHASE 2: Direct Preference Optimization\")\n",
        "            self.logger.info(\"=\" * 50)\n",
        "\n",
        "            dpo_dataset = self.data_service.load_dpo_dataset()\n",
        "            dpo_trainer = self.dpo_service.train(\n",
        "                self.model_service.model,\n",
        "                self.model_service.tokenizer,\n",
        "                dpo_dataset\n",
        "            )\n",
        "\n",
        "            # Step 4: Push to Hub\n",
        "            self.logger.info(\"=\" * 50)\n",
        "            self.logger.info(\"PHASE 3: Publishing to HuggingFace Hub\")\n",
        "            self.logger.info(\"=\" * 50)\n",
        "\n",
        "            self.hub_service.push_to_hub(self.config.dpo.output_dir)\n",
        "\n",
        "            self.logger.info(\"Pipeline completed successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            # Removed wandb.finish()\n",
        "            pass # Added pass to keep finally block"
      ],
      "metadata": {
        "id": "45iFdiJfZKXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Main calling points\"\"\"\n",
        "def create_default_config():\n",
        "    \"\"\"Create and save default configuration\"\"\"\n",
        "    pipeline = VoiceFinetuningPipeline()\n",
        "    pipeline.save_config()\n",
        "    print(\"✅ Default configuration saved to pipeline_config.json\")\n",
        "    return pipeline\n",
        "\n",
        "def run_pipeline_with_defaults():\n",
        "    \"\"\"Run pipeline with default configuration - Perfect for Jupyter\"\"\"\n",
        "    print(\"🎙️ Starting Voice Fine-tuning Pipeline with Default Settings\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        pipeline = VoiceFinetuningPipeline()\n",
        "        pipeline.run_pipeline()\n",
        "        print(\"🎉 Pipeline completed successfully!\")\n",
        "        return pipeline\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pipeline failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def run_pipeline_with_config(config_path: str = \"pipeline_config.json\"):\n",
        "    \"\"\"Run pipeline with specified config file\"\"\"\n",
        "    print(f\"🎙️ Starting Voice Fine-tuning Pipeline with config: {config_path}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        pipeline = VoiceFinetuningPipeline(config_path)\n",
        "        pipeline.run_pipeline()\n",
        "        print(\"🎉 Pipeline completed successfully!\")\n",
        "        return pipeline\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pipeline failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def run_quick_demo():\n",
        "    \"\"\"Quick demo with minimal training steps - Great for testing\"\"\"\n",
        "    print(\"🎯 Running Quick Demo Mode\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    # Create quick demo config\n",
        "    demo_config = PipelineConfig()\n",
        "    demo_config.sft.max_steps = 10\n",
        "    demo_config.dpo.max_steps = 5\n",
        "    demo_config.sft.per_device_train_batch_size = 1\n",
        "    demo_config.dpo.per_device_train_batch_size = 1\n",
        "    demo_config.hub.repo_id = \"demo/voice-assistant-test\"\n",
        "\n",
        "    try:\n",
        "        pipeline = VoiceFinetuningPipeline()\n",
        "        pipeline.config = demo_config\n",
        "        pipeline.run_pipeline()\n",
        "        print(\"🎉 Demo completed successfully!\")\n",
        "        return pipeline\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Demo failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def setup_custom_pipeline(**kwargs):\n",
        "    \"\"\"Setup custom pipeline with keyword arguments\"\"\"\n",
        "    print(\"🔧 Setting up Custom Pipeline Configuration\")\n",
        "    print(\"=\" * 45)\n",
        "\n",
        "    config = PipelineConfig()\n",
        "\n",
        "    # Update config with provided kwargs\n",
        "    for key, value in kwargs.items():\n",
        "        if hasattr(config, key):\n",
        "            setattr(config, key, value)\n",
        "        else:\n",
        "            # Handle nested config updates\n",
        "            if '.' in key:\n",
        "                section, param = key.split('.', 1)\n",
        "                if hasattr(config, section):\n",
        "                    section_config = getattr(config, section)\n",
        "                    if hasattr(section_config, param):\n",
        "                        setattr(section_config, param, value)\n",
        "\n",
        "    try:\n",
        "        pipeline = VoiceFinetuningPipeline()\n",
        "        pipeline.config = config\n",
        "        pipeline.run_pipeline()\n",
        "        print(\"🎉 Custom pipeline completed successfully!\")\n",
        "        return pipeline\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Custom pipeline failed: {str(e)}\")\n",
        "        raise\n",
        "def main():\n",
        "    \"\"\"Main function called\"\"\"\n",
        "    return run_pipeline_with_defaults()"
      ],
      "metadata": {
        "id": "_yfezL1WZMNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Quick start - runs immediately with defaults\n",
        "# pipeline = main()\n",
        "\n",
        "# # Or run specific functions:\n",
        "# pipeline = run_pipeline_with_defaults()\n",
        "\n",
        "# Quick demo for testing\n",
        "pipeline = run_quick_demo()\n",
        "\n",
        "# Custom parameters inline\n",
        "# pipeline = setup_custom_pipeline(\n",
        "#     hub_repo_id=\"rahulsamant37/my-voice-model\",\n",
        "#     sft_max_steps=50,\n",
        "#     dpo_max_steps=25\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "F1br8tBmbq8Q",
        "outputId": "dab26808-3c9e-4645-e567-13b4fbb40e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-21 18:27:13,507 - voice_pipeline - INFO - Starting Voice Fine-tuning Pipeline\n",
            "INFO:voice_pipeline:Starting Voice Fine-tuning Pipeline\n",
            "2025-06-21 18:27:13,509 - voice_pipeline - INFO - Loading base model: unsloth/llama-3-8b-bnb-4bit\n",
            "INFO:voice_pipeline:Loading base model: unsloth/llama-3-8b-bnb-4bit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Running Quick Demo Mode\n",
            "==============================\n",
            "==((====))==  Unsloth 2025.6.4: Fast Llama patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-21 18:27:15,572 - voice_pipeline - ERROR - Pipeline failed: LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'\n",
            "ERROR:voice_pipeline:Pipeline failed: LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Demo failed: LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-36-4160788123.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Quick demo for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_quick_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Custom parameters inline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-35-442267556.py\u001b[0m in \u001b[0;36mrun_quick_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoiceFinetuningPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🎉 Demo completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-34-3378305675.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Step 1: Load base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-30-1227197420.py\u001b[0m in \u001b[0;36mload_base_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading base model: {self.config.model.base_model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, **kwargs)\u001b[0m\n\u001b[1;32m   1870\u001b[0m             )\n\u001b[1;32m   1871\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1872\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1873\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4506\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4507\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4508\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4510\u001b[0m         \u001b[0;31m# Make sure to tie the weights correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: LlamaForCausalLM.__init__() got an unexpected keyword argument 'llm_int8_enable_fp32_cpu_offload'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lxcEfhBfdgtI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}