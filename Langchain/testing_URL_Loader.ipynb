{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f219bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61b8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using PyPDFLoader with temporary file\n",
    "def load_pdf_from_url_method1(url):\n",
    "    \"\"\"\n",
    "    Load PDF from URL using PyPDFLoader with temporary file\n",
    "    \"\"\"\n",
    "    # Download the PDF content\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Create a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "        tmp_file.write(response.content)\n",
    "        tmp_file_path = tmp_file.name\n",
    "    \n",
    "    try:\n",
    "        # Load the PDF using PyPDFLoader\n",
    "        loader = PyPDFLoader(tmp_file_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        return documents\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        os.unlink(tmp_file_path)\n",
    "\n",
    "# Method 2: Using OnlinePDFLoader (if available)\n",
    "def load_pdf_from_url_method2(url):\n",
    "    \"\"\"\n",
    "    Load PDF from URL using OnlinePDFLoader\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from langchain.document_loaders import OnlinePDFLoader\n",
    "        loader = OnlinePDFLoader(url)\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    except ImportError:\n",
    "        print(\"OnlinePDFLoader not available. Install with: pip install langchain[pdf]\")\n",
    "        return None\n",
    "\n",
    "# Method 3: Using UnstructuredPDFLoader with URL\n",
    "def load_pdf_from_url_method3(url):\n",
    "    \"\"\"\n",
    "    Load PDF from URL using UnstructuredPDFLoader\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from langchain.document_loaders import UnstructuredPDFLoader\n",
    "        import io\n",
    "        \n",
    "        # Download the PDF content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "            tmp_file.write(response.content)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        try:\n",
    "            # Load using UnstructuredPDFLoader\n",
    "            loader = UnstructuredPDFLoader(tmp_file_path)\n",
    "            documents = loader.load()\n",
    "            return documents\n",
    "        finally:\n",
    "            # Clean up\n",
    "            os.unlink(tmp_file_path)\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"UnstructuredPDFLoader not available. Install with: pip install unstructured[pdf]\")\n",
    "        return None\n",
    "\n",
    "# Method 4: Using WebBaseLoader for web-based PDFs\n",
    "def load_pdf_from_url_method4(url):\n",
    "    \"\"\"\n",
    "    Load PDF from URL using WebBaseLoader\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from langchain.document_loaders import WebBaseLoader\n",
    "        loader = WebBaseLoader(url)\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"WebBaseLoader failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8577d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to demonstrate usage\n",
    "def main():\n",
    "    url = \"https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D\"\n",
    "    \n",
    "    print(\"Loading PDF from URL...\")\n",
    "    \n",
    "    # Try Method 1 (most reliable)\n",
    "    try:\n",
    "        documents = load_pdf_from_url_method1(url)\n",
    "        print(f\"Method 1 - Successfully loaded {len(documents)} pages\")\n",
    "        \n",
    "        # Print first page content (truncated)\n",
    "        if documents:\n",
    "            print(f\"First page content preview:\")\n",
    "            print(documents[0].page_content[:500] + \"...\" if len(documents[0].page_content) > 500 else documents[0].page_content)\n",
    "            \n",
    "            # Split documents into chunks for further processing\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len,\n",
    "            )\n",
    "            \n",
    "            chunks = text_splitter.split_documents(documents)\n",
    "            print(f\"Split into {len(chunks)} chunks\")\n",
    "            \n",
    "            return documents, chunks\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Method 1 failed: {e}\")\n",
    "    \n",
    "    # Try Method 2 as fallback\n",
    "    try:\n",
    "        documents = load_pdf_from_url_method2(url)\n",
    "        if documents:\n",
    "            print(f\"Method 2 - Successfully loaded {len(documents)} pages\")\n",
    "            return documents, None\n",
    "    except Exception as e:\n",
    "        print(f\"Method 2 failed: {e}\")\n",
    "    \n",
    "    # Try Method 3 as fallback\n",
    "    try:\n",
    "        documents = load_pdf_from_url_method3(url)\n",
    "        if documents:\n",
    "            print(f\"Method 3 - Successfully loaded {len(documents)} pages\")\n",
    "            return documents, None\n",
    "    except Exception as e:\n",
    "        print(f\"Method 3 failed: {e}\")\n",
    "    \n",
    "    print(\"All methods failed to load the PDF\")\n",
    "    return None, None\n",
    "\n",
    "# Example usage with additional processing\n",
    "def process_pdf_for_qa(url):\n",
    "    \"\"\"\n",
    "    Complete pipeline for loading and processing PDF for Q&A with retry logic\n",
    "    \"\"\"\n",
    "    documents, chunks = main()\n",
    "    \n",
    "    if not documents:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "        from langchain.vectorstores import FAISS\n",
    "        \n",
    "        # Create embeddings with retry logic\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "        \n",
    "        # Process in smaller batches to avoid quota limits\n",
    "        if chunks:\n",
    "            # Process chunks in smaller batches\n",
    "            batch_size = 10  # Reduce batch size\n",
    "            vectorstore = None\n",
    "            \n",
    "            for i in range(0, len(chunks), batch_size):\n",
    "                batch_chunks = chunks[i:i+batch_size]\n",
    "                print(f\"Processing batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}\")\n",
    "                \n",
    "                try:\n",
    "                    if vectorstore is None:\n",
    "                        vectorstore = FAISS.from_documents(batch_chunks, embeddings)\n",
    "                    else:\n",
    "                        batch_vectorstore = FAISS.from_documents(batch_chunks, embeddings)\n",
    "                        vectorstore.merge_from(batch_vectorstore)\n",
    "                    \n",
    "                    # Add delay between batches to respect rate limits\n",
    "                    time.sleep(2)  # 2 second delay\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
    "                    print(\"Waiting 60 seconds before retrying...\")\n",
    "                    time.sleep(60)  # Wait 1 minute on error\n",
    "                    \n",
    "                    # Retry the batch\n",
    "                    try:\n",
    "                        if vectorstore is None:\n",
    "                            vectorstore = FAISS.from_documents(batch_chunks, embeddings)\n",
    "                        else:\n",
    "                            batch_vectorstore = FAISS.from_documents(batch_chunks, embeddings)\n",
    "                            vectorstore.merge_from(batch_vectorstore)\n",
    "                    except Exception as retry_error:\n",
    "                        print(f\"Retry failed for batch {i//batch_size + 1}: {retry_error}\")\n",
    "                        continue\n",
    "            \n",
    "            print(\"Created vector store from document chunks with rate limiting\")\n",
    "            return vectorstore\n",
    "        else:\n",
    "            # Process documents in smaller batches\n",
    "            batch_size = 5\n",
    "            vectorstore = None\n",
    "            \n",
    "            for i in range(0, len(documents), batch_size):\n",
    "                batch_docs = documents[i:i+batch_size]\n",
    "                \n",
    "                try:\n",
    "                    if vectorstore is None:\n",
    "                        vectorstore = FAISS.from_documents(batch_docs, embeddings)\n",
    "                    else:\n",
    "                        batch_vectorstore = FAISS.from_documents(batch_docs, embeddings)\n",
    "                        vectorstore.merge_from(batch_vectorstore)\n",
    "                    \n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing document batch: {e}\")\n",
    "                    time.sleep(60)\n",
    "            \n",
    "            print(\"Created vector store from documents with rate limiting\")\n",
    "            return vectorstore\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"GEMINI embeddings not available. Install with: pip install langchain_google_genai\")\n",
    "        return documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c4ad43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from URL...\n",
      "Method 1 - Successfully loaded 25 pages\n",
      "First page content preview:\n",
      "National Insurance Co. Ltd. \n",
      "Premises No. 18-0374, Plot no. CBD-81,  \n",
      "New Town, Kolkata - 700156 \n",
      "Page 1 of 25 National Parivar Mediclaim Plus Policy \n",
      "UIN: NICHLIP25039V032425 \n",
      " \n",
      "National Insurance Company Limited \n",
      "     CIN - U10200WB1906GOI001713 IRDAI Regn. No. – 58 \n",
      " \n",
      "           Issuing Office \n",
      "National Parivar Mediclaim Plus Policy  \n",
      " \n",
      "Whereas the Proposer designated in the schedule hereto has by a Proposal together with Declaration, which shall be the basis of \n",
      "this contract and is deemed t...\n",
      "Split into 143 chunks\n"
     ]
    }
   ],
   "source": [
    "documents, chunks = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efab8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from URL...\n",
      "Method 1 - Successfully loaded 25 pages\n",
      "First page content preview:\n",
      "National Insurance Co. Ltd. \n",
      "Premises No. 18-0374, Plot no. CBD-81,  \n",
      "New Town, Kolkata - 700156 \n",
      "Page 1 of 25 National Parivar Mediclaim Plus Policy \n",
      "UIN: NICHLIP25039V032425 \n",
      " \n",
      "National Insurance Company Limited \n",
      "     CIN - U10200WB1906GOI001713 IRDAI Regn. No. – 58 \n",
      " \n",
      "           Issuing Office \n",
      "National Parivar Mediclaim Plus Policy  \n",
      " \n",
      "Whereas the Proposer designated in the schedule hereto has by a Proposal together with Declaration, which shall be the basis of \n",
      "this contract and is deemed t...\n",
      "Split into 143 chunks\n",
      "Processing batch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading faiss with AVX2 support.\n",
      "INFO: Successfully loaded faiss with AVX2 support.\n",
      "INFO: Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Error processing batch 11: Error embedding content: 429 Resource has been exhausted (e.g. check quota).\n",
      "Waiting 60 seconds before retrying...\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Created vector store from document chunks with rate limiting\n"
     ]
    }
   ],
   "source": [
    "# Required installations:\n",
    "# pip install langchain pypdf requests\n",
    "# Optional: pip install unstructured[pdf] openai faiss-cpu\n",
    "\n",
    "url = \"https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D\"    \n",
    "# Or for Q&A processing:\n",
    "vectorstore = process_pdf_for_qa(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0cc0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\n",
    "    \"What is the waiting period for pre-existing diseases (PED) to be covered?\",\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7481ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* provided the Policy has been continuously renewed with the Company without a break. Expenses payable are subject to the limit \n",
      "stated in the Table of Benefits.   \n",
      "  \n",
      "4 EXCLUSIONS  \n",
      "The Company shall not be liable to make any payment by the Policy, in respect of any expenses incurred in connection with or in \n",
      "respect of: \n",
      " \n",
      "4.1. Pre-Existing Diseases (Excl 01) \n",
      "a) Expenses related to the treatment of a Pre-Existing Disease (PED) and its direct complications shall be excluded until the \n",
      "expiry of thirty six (36) months of continuous coverage after the date of inception of the first policy with us.  \n",
      "b) In case of enhancement of sum insured the exclusion shall apply afresh to the extent of sum insured increase.  \n",
      "c) If the Insured Person is continuously covered without any break as defined under the portability norms of the extant IRDAI \n",
      "(Health Insurance) Regulations then waiting period for the same would be reduced to the extent of prior coverage. [{'producer': 'Microsoft® Word LTSC', 'creator': 'Microsoft® Word LTSC', 'creationdate': '2025-02-11T11:39:19+05:30', 'title': 'National Parivar Mediclaim Plus Policy (NPMPP)', 'author': 'Avishek Banerjee', 'moddate': '2025-02-11T11:39:19+05:30', 'source': 'C:\\\\Users\\\\Rahul\\\\AppData\\\\Local\\\\Temp\\\\tmphi0fl4s2.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9'}]\n"
     ]
    }
   ],
   "source": [
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e513bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daily-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
