{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
       "0    842302         M  ...                  0.11890          NaN\n",
       "1    842517         M  ...                  0.08902          NaN\n",
       "2  84300903         M  ...                  0.08758          NaN\n",
       "3  84348301         M  ...                  0.17300          NaN\n",
       "4  84358402         M  ...                  0.07678          NaN\n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>4.885000</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>542.200000</td>\n",
       "      <td>0.031130</td>\n",
       "      <td>0.135400</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  ...  fractal_dimension_worst  Unnamed: 32\n",
       "count  5.690000e+02   569.000000  ...               569.000000          0.0\n",
       "mean   3.037183e+07    14.127292  ...                 0.083946          NaN\n",
       "std    1.250206e+08     3.524049  ...                 0.018061          NaN\n",
       "min    8.670000e+03     6.981000  ...                 0.055040          NaN\n",
       "25%    8.692180e+05    11.700000  ...                 0.071460          NaN\n",
       "50%    9.060240e+05    13.370000  ...                 0.080040          NaN\n",
       "75%    8.813129e+06    15.780000  ...                 0.092080          NaN\n",
       "max    9.113205e+08    28.110000  ...                 0.207500          NaN\n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 33)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"id\",\"Unnamed: 32\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>M</td>\n",
       "      <td>19.00</td>\n",
       "      <td>18.91</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.08217</td>\n",
       "      <td>0.08028</td>\n",
       "      <td>0.092710</td>\n",
       "      <td>0.056270</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.05044</td>\n",
       "      <td>0.6896</td>\n",
       "      <td>1.342</td>\n",
       "      <td>5.216</td>\n",
       "      <td>81.23</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.027310</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.013610</td>\n",
       "      <td>0.02030</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>22.32</td>\n",
       "      <td>25.73</td>\n",
       "      <td>148.20</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.06541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>B</td>\n",
       "      <td>14.97</td>\n",
       "      <td>16.95</td>\n",
       "      <td>96.22</td>\n",
       "      <td>685.9</td>\n",
       "      <td>0.09855</td>\n",
       "      <td>0.07885</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>0.037810</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.05650</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>1.217</td>\n",
       "      <td>1.893</td>\n",
       "      <td>24.28</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.01350</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>16.11</td>\n",
       "      <td>23.00</td>\n",
       "      <td>104.60</td>\n",
       "      <td>793.7</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.16370</td>\n",
       "      <td>0.066480</td>\n",
       "      <td>0.084850</td>\n",
       "      <td>0.2404</td>\n",
       "      <td>0.06428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>B</td>\n",
       "      <td>12.00</td>\n",
       "      <td>28.23</td>\n",
       "      <td>76.77</td>\n",
       "      <td>442.5</td>\n",
       "      <td>0.08437</td>\n",
       "      <td>0.06450</td>\n",
       "      <td>0.040550</td>\n",
       "      <td>0.019450</td>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.06104</td>\n",
       "      <td>0.1912</td>\n",
       "      <td>1.705</td>\n",
       "      <td>1.516</td>\n",
       "      <td>13.86</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.01745</td>\n",
       "      <td>0.004302</td>\n",
       "      <td>13.09</td>\n",
       "      <td>37.88</td>\n",
       "      <td>85.07</td>\n",
       "      <td>523.7</td>\n",
       "      <td>0.12080</td>\n",
       "      <td>0.18560</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.071160</td>\n",
       "      <td>0.2447</td>\n",
       "      <td>0.08194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M</td>\n",
       "      <td>16.02</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.032990</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>0.3795</td>\n",
       "      <td>1.187</td>\n",
       "      <td>2.466</td>\n",
       "      <td>40.51</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>0.01460</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>19.19</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.099750</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>M</td>\n",
       "      <td>19.59</td>\n",
       "      <td>18.15</td>\n",
       "      <td>130.70</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.16660</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>0.7364</td>\n",
       "      <td>1.048</td>\n",
       "      <td>4.792</td>\n",
       "      <td>97.07</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.022770</td>\n",
       "      <td>0.040290</td>\n",
       "      <td>0.013030</td>\n",
       "      <td>0.01686</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>26.73</td>\n",
       "      <td>26.39</td>\n",
       "      <td>174.90</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>0.14380</td>\n",
       "      <td>0.38460</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.3643</td>\n",
       "      <td>0.09223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>B</td>\n",
       "      <td>13.37</td>\n",
       "      <td>16.39</td>\n",
       "      <td>86.10</td>\n",
       "      <td>553.5</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.07325</td>\n",
       "      <td>0.080920</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>0.05823</td>\n",
       "      <td>0.1639</td>\n",
       "      <td>1.140</td>\n",
       "      <td>1.223</td>\n",
       "      <td>14.66</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.049570</td>\n",
       "      <td>0.010380</td>\n",
       "      <td>0.01208</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>14.26</td>\n",
       "      <td>22.75</td>\n",
       "      <td>91.99</td>\n",
       "      <td>632.1</td>\n",
       "      <td>0.10250</td>\n",
       "      <td>0.25310</td>\n",
       "      <td>0.330800</td>\n",
       "      <td>0.089780</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.07628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>B</td>\n",
       "      <td>13.01</td>\n",
       "      <td>22.22</td>\n",
       "      <td>82.01</td>\n",
       "      <td>526.4</td>\n",
       "      <td>0.06251</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.05234</td>\n",
       "      <td>0.1731</td>\n",
       "      <td>1.142</td>\n",
       "      <td>1.101</td>\n",
       "      <td>14.34</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.01613</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>14.00</td>\n",
       "      <td>29.02</td>\n",
       "      <td>88.18</td>\n",
       "      <td>608.8</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.03432</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.05843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n",
       "127         M        19.00  ...          0.2841                  0.06541\n",
       "169         B        14.97  ...          0.2404                  0.06428\n",
       "452         B        12.00  ...          0.2447                  0.08194\n",
       "10          M        16.02  ...          0.2948                  0.08452\n",
       "162         M        19.59  ...          0.3643                  0.09223\n",
       "124         B        13.37  ...          0.2048                  0.07628\n",
       "178         B        13.01  ...          0.2295                  0.05843\n",
       "\n",
       "[7 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding on Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy arrays to Pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleNN():\n",
    "    def __init__(self, X):\n",
    "        self.weights = torch.rand(X.shape[1],1,dtype=torch.float64, requires_grad=True)\n",
    "        self.bais = torch.zeros(1,dtype=torch.float64,requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z = torch.matmul(X, self.weights) + self.bais\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        return y_pred\n",
    "    \n",
    "    def loss_function(self, y_pred, y):\n",
    "        loss = -(y_train_tensor * torch.log(y_pred) + (1 - y_train_tensor) * torch.log(1 - y_pred)).mean()\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1001, Loss: 4.070101670038381\n",
      "Epochs: 1001, Loss: 3.859799999550753\n",
      "Epochs: 1001, Loss: 3.648353237260537\n",
      "Epochs: 1001, Loss: 3.4393388998591723\n",
      "Epochs: 1001, Loss: 3.233024793279373\n",
      "Epochs: 1001, Loss: 3.029677173529173\n",
      "Epochs: 1001, Loss: 2.82969819107592\n",
      "Epochs: 1001, Loss: 2.6335707418668095\n",
      "Epochs: 1001, Loss: 2.4418499628401618\n",
      "Epochs: 1001, Loss: 2.2551780591510546\n",
      "Epochs: 1001, Loss: 2.0743036192974578\n",
      "Epochs: 1001, Loss: 1.9001174602303446\n",
      "Epochs: 1001, Loss: 1.73370371414193\n",
      "Epochs: 1001, Loss: 1.5764001653945916\n",
      "Epochs: 1001, Loss: 1.429847992081205\n",
      "Epochs: 1001, Loss: 1.295988685279744\n",
      "Epochs: 1001, Loss: 1.176926052361148\n",
      "Epochs: 1001, Loss: 1.0745416258545293\n",
      "Epochs: 1001, Loss: 0.9898654826701548\n",
      "Epochs: 1001, Loss: 0.9225244951559133\n",
      "Epochs: 1001, Loss: 0.8707145991916722\n",
      "Epochs: 1001, Loss: 0.8317238851594102\n",
      "Epochs: 1001, Loss: 0.8026171281890321\n",
      "Epochs: 1001, Loss: 0.7807531676058082\n",
      "Epochs: 1001, Loss: 0.7640384654657808\n",
      "Epochs: 1001, Loss: 0.7509582562408615\n",
      "Epochs: 1001, Loss: 0.7404795463065811\n",
      "Epochs: 1001, Loss: 0.7319164902316136\n",
      "Epochs: 1001, Loss: 0.7248119790468276\n",
      "Epochs: 1001, Loss: 0.7188526238551022\n",
      "Epochs: 1001, Loss: 0.7138142612741026\n",
      "Epochs: 1001, Loss: 0.7095291757336368\n",
      "Epochs: 1001, Loss: 0.705866938421815\n",
      "Epochs: 1001, Loss: 0.7027231856720026\n",
      "Epochs: 1001, Loss: 0.7000128392799334\n",
      "Epochs: 1001, Loss: 0.6976657712308219\n",
      "Epochs: 1001, Loss: 0.6956238266402082\n",
      "Epochs: 1001, Loss: 0.6938386323037077\n",
      "Epochs: 1001, Loss: 0.69226989280601\n",
      "Epochs: 1001, Loss: 0.690884017134584\n",
      "Epochs: 1001, Loss: 0.6896529890355416\n",
      "Epochs: 1001, Loss: 0.6885534288563078\n",
      "Epochs: 1001, Loss: 0.6875658117228257\n",
      "Epochs: 1001, Loss: 0.6866738158309196\n",
      "Epochs: 1001, Loss: 0.6858637798266611\n",
      "Epochs: 1001, Loss: 0.6851242517427596\n",
      "Epochs: 1001, Loss: 0.6844456146338569\n",
      "Epochs: 1001, Loss: 0.6838197762792885\n",
      "Epochs: 1001, Loss: 0.6832399122413547\n",
      "Epochs: 1001, Loss: 0.6827002532374384\n",
      "Epochs: 1001, Loss: 0.6821959092327107\n",
      "Epochs: 1001, Loss: 0.6817227239063324\n",
      "Epochs: 1001, Loss: 0.6812771542070525\n",
      "Epochs: 1001, Loss: 0.6808561706137408\n",
      "Epochs: 1001, Loss: 0.680457174472632\n",
      "Epochs: 1001, Loss: 0.6800779294152337\n",
      "Epochs: 1001, Loss: 0.6797165043868715\n",
      "Epochs: 1001, Loss: 0.6793712262519825\n",
      "Epochs: 1001, Loss: 0.6790406403027647\n",
      "Epochs: 1001, Loss: 0.6787234772951002\n",
      "Epochs: 1001, Loss: 0.6784186258804152\n",
      "Epochs: 1001, Loss: 0.6781251095033125\n",
      "Epochs: 1001, Loss: 0.677842067000017\n",
      "Epochs: 1001, Loss: 0.6775687362682393\n",
      "Epochs: 1001, Loss: 0.677304440490234\n",
      "Epochs: 1001, Loss: 0.6770485764820116\n",
      "Epochs: 1001, Loss: 0.6768006048164279\n",
      "Epochs: 1001, Loss: 0.6765600414291922\n",
      "Epochs: 1001, Loss: 0.6763264504671693\n",
      "Epochs: 1001, Loss: 0.6760994381796576\n",
      "Epochs: 1001, Loss: 0.6758786476872849\n",
      "Epochs: 1001, Loss: 0.6756637544910892\n",
      "Epochs: 1001, Loss: 0.6754544626073402\n",
      "Epochs: 1001, Loss: 0.6752505012326252\n",
      "Epochs: 1001, Loss: 0.675051621859363\n",
      "Epochs: 1001, Loss: 0.6748575957748612\n",
      "Epochs: 1001, Loss: 0.6746682118877442\n",
      "Epochs: 1001, Loss: 0.6744832748344751\n",
      "Epochs: 1001, Loss: 0.6743026033260884\n",
      "Epochs: 1001, Loss: 0.6741260287014059\n",
      "Epochs: 1001, Loss: 0.6739533936581524\n",
      "Epochs: 1001, Loss: 0.6737845511376774\n",
      "Epochs: 1001, Loss: 0.6736193633426064\n",
      "Epochs: 1001, Loss: 0.6734577008697611\n",
      "Epochs: 1001, Loss: 0.6732994419432458\n",
      "Epochs: 1001, Loss: 0.6731444717347477\n",
      "Epochs: 1001, Loss: 0.6729926817599167\n",
      "Epochs: 1001, Loss: 0.6728439693412442\n",
      "Epochs: 1001, Loss: 0.6726982371291619\n",
      "Epochs: 1001, Loss: 0.6725553926742088\n",
      "Epochs: 1001, Loss: 0.6724153480440709\n",
      "Epochs: 1001, Loss: 0.6722780194801051\n",
      "Epochs: 1001, Loss: 0.6721433270886771\n",
      "Epochs: 1001, Loss: 0.6720111945632306\n",
      "Epochs: 1001, Loss: 0.6718815489335331\n",
      "Epochs: 1001, Loss: 0.671754320338993\n",
      "Epochs: 1001, Loss: 0.6716294418233192\n",
      "Epochs: 1001, Loss: 0.6715068491481514\n",
      "Epochs: 1001, Loss: 0.6713864806235531\n",
      "Epochs: 1001, Loss: 0.6712682769535384\n",
      "Epochs: 1001, Loss: 0.6711521810950087\n",
      "Epochs: 1001, Loss: 0.671038138128671\n",
      "Epochs: 1001, Loss: 0.6709260951406832\n",
      "Epochs: 1001, Loss: 0.6708160011139107\n",
      "Epochs: 1001, Loss: 0.6707078068278087\n",
      "Epochs: 1001, Loss: 0.6706014647660667\n",
      "Epochs: 1001, Loss: 0.6704969290312363\n",
      "Epochs: 1001, Loss: 0.6703941552656585\n",
      "Epochs: 1001, Loss: 0.6702931005780867\n",
      "Epochs: 1001, Loss: 0.6701937234754567\n",
      "Epochs: 1001, Loss: 0.6700959837993291\n",
      "Epochs: 1001, Loss: 0.669999842666569\n",
      "Epochs: 1001, Loss: 0.6699052624138832\n",
      "Epochs: 1001, Loss: 0.6698122065458714\n",
      "Epochs: 1001, Loss: 0.6697206396862833\n",
      "Epochs: 1001, Loss: 0.6696305275322119\n",
      "Epochs: 1001, Loss: 0.6695418368109716\n",
      "Epochs: 1001, Loss: 0.6694545352394434\n",
      "Epochs: 1001, Loss: 0.6693685914856901\n",
      "Epochs: 1001, Loss: 0.6692839751326576\n",
      "Epochs: 1001, Loss: 0.6692006566438075\n",
      "Epochs: 1001, Loss: 0.6691186073305322\n",
      "Epochs: 1001, Loss: 0.6690377993212198\n",
      "Epochs: 1001, Loss: 0.6689582055318545\n",
      "Epochs: 1001, Loss: 0.6688797996380401\n",
      "Epochs: 1001, Loss: 0.6688025560483509\n",
      "Epochs: 1001, Loss: 0.6687264498789213\n",
      "Epochs: 1001, Loss: 0.6686514569291914\n",
      "Epochs: 1001, Loss: 0.6685775536587362\n",
      "Epochs: 1001, Loss: 0.6685047171651098\n",
      "Epochs: 1001, Loss: 0.6684329251626435\n",
      "Epochs: 1001, Loss: 0.6683621559621385\n",
      "Epochs: 1001, Loss: 0.6682923884514075\n",
      "Epochs: 1001, Loss: 0.6682236020766064\n",
      "Epochs: 1001, Loss: 0.6681557768243255\n",
      "Epochs: 1001, Loss: 0.6680888932043854\n",
      "Epochs: 1001, Loss: 0.6680229322333116\n",
      "Epochs: 1001, Loss: 0.6679578754184445\n",
      "Epochs: 1001, Loss: 0.6678937047426583\n",
      "Epochs: 1001, Loss: 0.6678304026496521\n",
      "Epochs: 1001, Loss: 0.6677679520297932\n",
      "Epochs: 1001, Loss: 0.6677063362064791\n",
      "Epochs: 1001, Loss: 0.6676455389229967\n",
      "Epochs: 1001, Loss: 0.6675855443298575\n",
      "Epochs: 1001, Loss: 0.667526336972583\n",
      "Epochs: 1001, Loss: 0.6674679017799251\n",
      "Epochs: 1001, Loss: 0.6674102240524971\n",
      "Epochs: 1001, Loss: 0.6673532894518033\n",
      "Epochs: 1001, Loss: 0.667297083989645\n",
      "Epochs: 1001, Loss: 0.6672415940178914\n",
      "Epochs: 1001, Loss: 0.6671868062185965\n",
      "Epochs: 1001, Loss: 0.6671327075944514\n",
      "Epochs: 1001, Loss: 0.6670792854595561\n",
      "Epochs: 1001, Loss: 0.6670265274304987\n",
      "Epochs: 1001, Loss: 0.6669744214177308\n",
      "Epochs: 1001, Loss: 0.6669229556172256\n",
      "Epochs: 1001, Loss: 0.6668721185024108\n",
      "Epochs: 1001, Loss: 0.6668218988163621\n",
      "Epochs: 1001, Loss: 0.6667722855642506\n",
      "Epochs: 1001, Loss: 0.6667232680060334\n",
      "Epochs: 1001, Loss: 0.6666748356493784\n",
      "Epochs: 1001, Loss: 0.6666269782428129\n",
      "Epochs: 1001, Loss: 0.6665796857690942\n",
      "Epochs: 1001, Loss: 0.6665329484387831\n",
      "Epochs: 1001, Loss: 0.6664867566840249\n",
      "Epochs: 1001, Loss: 0.6664411011525222\n",
      "Epochs: 1001, Loss: 0.6663959727016977\n",
      "Epochs: 1001, Loss: 0.6663513623930344\n",
      "Epochs: 1001, Loss: 0.6663072614865958\n",
      "Epochs: 1001, Loss: 0.6662636614357103\n",
      "Epochs: 1001, Loss: 0.666220553881821\n",
      "Epochs: 1001, Loss: 0.6661779306494924\n",
      "Epochs: 1001, Loss: 0.6661357837415682\n",
      "Epochs: 1001, Loss: 0.6660941053344769\n",
      "Epochs: 1001, Loss: 0.6660528877736781\n",
      "Epochs: 1001, Loss: 0.6660121235692469\n",
      "Epochs: 1001, Loss: 0.6659718053915892\n",
      "Epochs: 1001, Loss: 0.6659319260672865\n",
      "Epochs: 1001, Loss: 0.6658924785750631\n",
      "Epochs: 1001, Loss: 0.6658534560418735\n",
      "Epochs: 1001, Loss: 0.665814851739105\n",
      "Epochs: 1001, Loss: 0.6657766590788926\n",
      "Epochs: 1001, Loss: 0.6657388716105409\n",
      "Epochs: 1001, Loss: 0.6657014830170529\n",
      "Epochs: 1001, Loss: 0.6656644871117563\n",
      "Epochs: 1001, Loss: 0.6656278778350319\n",
      "Epochs: 1001, Loss: 0.665591649251133\n",
      "Epochs: 1001, Loss: 0.6655557955450995\n",
      "Epochs: 1001, Loss: 0.6655203110197576\n",
      "Epochs: 1001, Loss: 0.6654851900928094\n",
      "Epochs: 1001, Loss: 0.6654504272940007\n",
      "Epochs: 1001, Loss: 0.6654160172623748\n",
      "Epochs: 1001, Loss: 0.6653819547435986\n",
      "Epochs: 1001, Loss: 0.6653482345873686\n",
      "Epochs: 1001, Loss: 0.6653148517448886\n",
      "Epochs: 1001, Loss: 0.665281801266417\n",
      "Epochs: 1001, Loss: 0.6652490782988841\n",
      "Epochs: 1001, Loss: 0.6652166780835754\n",
      "Epochs: 1001, Loss: 0.6651845959538799\n",
      "Epochs: 1001, Loss: 0.6651528273330989\n",
      "Epochs: 1001, Loss: 0.6651213677323178\n",
      "Epochs: 1001, Loss: 0.6650902127483345\n",
      "Epochs: 1001, Loss: 0.6650593580616455\n",
      "Epochs: 1001, Loss: 0.6650287994344874\n",
      "Epochs: 1001, Loss: 0.6649985327089303\n",
      "Epochs: 1001, Loss: 0.6649685538050247\n",
      "Epochs: 1001, Loss: 0.664938858718997\n",
      "Epochs: 1001, Loss: 0.6649094435214947\n",
      "Epochs: 1001, Loss: 0.6648803043558796\n",
      "Epochs: 1001, Loss: 0.6648514374365634\n",
      "Epochs: 1001, Loss: 0.6648228390473914\n",
      "Epochs: 1001, Loss: 0.6647945055400676\n",
      "Epochs: 1001, Loss: 0.6647664333326208\n",
      "Epochs: 1001, Loss: 0.664738618907914\n",
      "Epochs: 1001, Loss: 0.6647110588121887\n",
      "Epochs: 1001, Loss: 0.6646837496536523\n",
      "Epochs: 1001, Loss: 0.6646566881010982\n",
      "Epochs: 1001, Loss: 0.664629870882565\n",
      "Epochs: 1001, Loss: 0.6646032947840284\n",
      "Epochs: 1001, Loss: 0.6645769566481285\n",
      "Epochs: 1001, Loss: 0.66455085337293\n",
      "Epochs: 1001, Loss: 0.6645249819107111\n",
      "Epochs: 1001, Loss: 0.6644993392667888\n",
      "Epochs: 1001, Loss: 0.6644739224983699\n",
      "Epochs: 1001, Loss: 0.6644487287134332\n",
      "Epochs: 1001, Loss: 0.6644237550696391\n",
      "Epochs: 1001, Loss: 0.6643989987732676\n",
      "Epochs: 1001, Loss: 0.664374457078183\n",
      "Epochs: 1001, Loss: 0.6643501272848241\n",
      "Epochs: 1001, Loss: 0.6643260067392195\n",
      "Epochs: 1001, Loss: 0.6643020928320288\n",
      "Epochs: 1001, Loss: 0.6642783829976056\n",
      "Epochs: 1001, Loss: 0.6642548747130858\n",
      "Epochs: 1001, Loss: 0.6642315654974968\n",
      "Epochs: 1001, Loss: 0.6642084529108895\n",
      "Epochs: 1001, Loss: 0.6641855345534915\n",
      "Epochs: 1001, Loss: 0.6641628080648802\n",
      "Epochs: 1001, Loss: 0.6641402711231771\n",
      "Epochs: 1001, Loss: 0.6641179214442612\n",
      "Epochs: 1001, Loss: 0.6640957567810015\n",
      "Epochs: 1001, Loss: 0.6640737749225076\n",
      "Epochs: 1001, Loss: 0.664051973693399\n",
      "Epochs: 1001, Loss: 0.66403035095309\n",
      "Epochs: 1001, Loss: 0.6640089045950961\n",
      "Epochs: 1001, Loss: 0.6639876325463512\n",
      "Epochs: 1001, Loss: 0.6639665327665448\n",
      "Epochs: 1001, Loss: 0.6639456032474738\n",
      "Epochs: 1001, Loss: 0.6639248420124088\n",
      "Epochs: 1001, Loss: 0.6639042471154769\n",
      "Epochs: 1001, Loss: 0.6638838166410576\n",
      "Epochs: 1001, Loss: 0.6638635487031921\n",
      "Epochs: 1001, Loss: 0.663843441445009\n",
      "Epochs: 1001, Loss: 0.66382349303816\n",
      "Epochs: 1001, Loss: 0.6638037016822717\n",
      "Epochs: 1001, Loss: 0.6637840656044072\n",
      "Epochs: 1001, Loss: 0.6637645830585412\n",
      "Epochs: 1001, Loss: 0.6637452523250483\n",
      "Epochs: 1001, Loss: 0.6637260717102011\n",
      "Epochs: 1001, Loss: 0.6637070395456799\n",
      "Epochs: 1001, Loss: 0.6636881541880945\n",
      "Epochs: 1001, Loss: 0.6636694140185151\n",
      "Epochs: 1001, Loss: 0.663650817442017\n",
      "Epochs: 1001, Loss: 0.6636323628872302\n",
      "Epochs: 1001, Loss: 0.6636140488059044\n",
      "Epochs: 1001, Loss: 0.6635958736724799\n",
      "Epochs: 1001, Loss: 0.6635778359836697\n",
      "Epochs: 1001, Loss: 0.6635599342580508\n",
      "Epochs: 1001, Loss: 0.6635421670356636\n",
      "Epochs: 1001, Loss: 0.6635245328776205\n",
      "Epochs: 1001, Loss: 0.6635070303657241\n",
      "Epochs: 1001, Loss: 0.6634896581020908\n",
      "Epochs: 1001, Loss: 0.6634724147087865\n",
      "Epochs: 1001, Loss: 0.663455298827466\n",
      "Epochs: 1001, Loss: 0.6634383091190246\n",
      "Epochs: 1001, Loss: 0.6634214442632526\n",
      "Epochs: 1001, Loss: 0.6634047029585003\n",
      "Epochs: 1001, Loss: 0.6633880839213492\n",
      "Epochs: 1001, Loss: 0.6633715858862904\n",
      "Epochs: 1001, Loss: 0.6633552076054092\n",
      "Epochs: 1001, Loss: 0.6633389478480761\n",
      "Epochs: 1001, Loss: 0.6633228054006461\n",
      "Epochs: 1001, Loss: 0.6633067790661625\n",
      "Epochs: 1001, Loss: 0.663290867664067\n",
      "Epochs: 1001, Loss: 0.6632750700299169\n",
      "Epochs: 1001, Loss: 0.6632593850151073\n",
      "Epochs: 1001, Loss: 0.663243811486599\n",
      "Epochs: 1001, Loss: 0.6632283483266531\n",
      "Epochs: 1001, Loss: 0.6632129944325688\n",
      "Epochs: 1001, Loss: 0.6631977487164289\n",
      "Epochs: 1001, Loss: 0.6631826101048497\n",
      "Epochs: 1001, Loss: 0.6631675775387346\n",
      "Epochs: 1001, Loss: 0.6631526499730359\n",
      "Epochs: 1001, Loss: 0.663137826376517\n",
      "Epochs: 1001, Loss: 0.6631231057315234\n",
      "Epochs: 1001, Loss: 0.6631084870337565\n",
      "Epochs: 1001, Loss: 0.6630939692920508\n",
      "Epochs: 1001, Loss: 0.6630795515281588\n",
      "Epochs: 1001, Loss: 0.6630652327765363\n",
      "Epochs: 1001, Loss: 0.6630510120841351\n",
      "Epochs: 1001, Loss: 0.6630368885101972\n",
      "Epochs: 1001, Loss: 0.6630228611260558\n",
      "Epochs: 1001, Loss: 0.6630089290149378\n",
      "Epochs: 1001, Loss: 0.6629950912717714\n",
      "Epochs: 1001, Loss: 0.6629813470029965\n",
      "Epochs: 1001, Loss: 0.6629676953263801\n",
      "Epochs: 1001, Loss: 0.6629541353708347\n",
      "Epochs: 1001, Loss: 0.6629406662762394\n",
      "Epochs: 1001, Loss: 0.6629272871932653\n",
      "Epochs: 1001, Loss: 0.6629139972832043\n",
      "Epochs: 1001, Loss: 0.6629007957178008\n",
      "Epochs: 1001, Loss: 0.6628876816790862\n",
      "Epochs: 1001, Loss: 0.6628746543592177\n",
      "Epochs: 1001, Loss: 0.6628617129603193\n",
      "Epochs: 1001, Loss: 0.6628488566943259\n",
      "Epochs: 1001, Loss: 0.6628360847828305\n",
      "Epochs: 1001, Loss: 0.6628233964569341\n",
      "Epochs: 1001, Loss: 0.662810790957098\n",
      "Epochs: 1001, Loss: 0.6627982675330006\n",
      "Epochs: 1001, Loss: 0.6627858254433943\n",
      "Epochs: 1001, Loss: 0.6627734639559666\n",
      "Epochs: 1001, Loss: 0.662761182347204\n",
      "Epochs: 1001, Loss: 0.6627489799022579\n",
      "Epochs: 1001, Loss: 0.6627368559148119\n",
      "Epochs: 1001, Loss: 0.6627248096869538\n",
      "Epochs: 1001, Loss: 0.6627128405290479\n",
      "Epochs: 1001, Loss: 0.6627009477596109\n",
      "Epochs: 1001, Loss: 0.6626891307051892\n",
      "Epochs: 1001, Loss: 0.6626773887002395\n",
      "Epochs: 1001, Loss: 0.6626657210870096\n",
      "Epochs: 1001, Loss: 0.6626541272154235\n",
      "Epochs: 1001, Loss: 0.6626426064429675\n",
      "Epochs: 1001, Loss: 0.6626311581345784\n",
      "Epochs: 1001, Loss: 0.6626197816625338\n",
      "Epochs: 1001, Loss: 0.6626084764063437\n",
      "Epochs: 1001, Loss: 0.6625972417526457\n",
      "Epochs: 1001, Loss: 0.6625860770951002\n",
      "Epochs: 1001, Loss: 0.662574981834288\n",
      "Epochs: 1001, Loss: 0.6625639553776107\n",
      "Epochs: 1001, Loss: 0.6625529971391919\n",
      "Epochs: 1001, Loss: 0.6625421065397791\n",
      "Epochs: 1001, Loss: 0.66253128300665\n",
      "Epochs: 1001, Loss: 0.6625205259735181\n",
      "Epochs: 1001, Loss: 0.6625098348804407\n",
      "Epochs: 1001, Loss: 0.6624992091737287\n",
      "Epochs: 1001, Loss: 0.6624886483058582\n",
      "Epochs: 1001, Loss: 0.6624781517353819\n",
      "Epochs: 1001, Loss: 0.6624677189268443\n",
      "Epochs: 1001, Loss: 0.6624573493506977\n",
      "Epochs: 1001, Loss: 0.6624470424832178\n",
      "Epochs: 1001, Loss: 0.6624367978064236\n",
      "Epochs: 1001, Loss: 0.6624266148079967\n",
      "Epochs: 1001, Loss: 0.6624164929812022\n",
      "Epochs: 1001, Loss: 0.6624064318248115\n",
      "Epochs: 1001, Loss: 0.6623964308430264\n",
      "Epochs: 1001, Loss: 0.6623864895454038\n",
      "Epochs: 1001, Loss: 0.6623766074467822\n",
      "Epochs: 1001, Loss: 0.6623667840672096\n",
      "Epochs: 1001, Loss: 0.6623570189318709\n",
      "Epochs: 1001, Loss: 0.6623473115710203\n",
      "Epochs: 1001, Loss: 0.6623376615199096\n",
      "Epochs: 1001, Loss: 0.6623280683187228\n",
      "Epochs: 1001, Loss: 0.6623185315125079\n",
      "Epochs: 1001, Loss: 0.6623090506511121\n",
      "Epochs: 1001, Loss: 0.6622996252891173\n",
      "Epochs: 1001, Loss: 0.6622902549857759\n",
      "Epochs: 1001, Loss: 0.6622809393049494\n",
      "Epochs: 1001, Loss: 0.6622716778150461\n",
      "Epochs: 1001, Loss: 0.6622624700889617\n",
      "Epochs: 1001, Loss: 0.6622533157040188\n",
      "Epochs: 1001, Loss: 0.6622442142419092\n",
      "Epochs: 1001, Loss: 0.6622351652886356\n",
      "Epochs: 1001, Loss: 0.6622261684344558\n",
      "Epochs: 1001, Loss: 0.6622172232738265\n",
      "Epochs: 1001, Loss: 0.6622083294053481\n",
      "Epochs: 1001, Loss: 0.6621994864317117\n",
      "Epochs: 1001, Loss: 0.662190693959645\n",
      "Epochs: 1001, Loss: 0.6621819515998609\n",
      "Epochs: 1001, Loss: 0.6621732589670052\n",
      "Epochs: 1001, Loss: 0.662164615679606\n",
      "Epochs: 1001, Loss: 0.6621560213600249\n",
      "Epochs: 1001, Loss: 0.6621474756344067\n",
      "Epochs: 1001, Loss: 0.6621389781326313\n",
      "Epochs: 1001, Loss: 0.6621305284882669\n",
      "Epochs: 1001, Loss: 0.6621221263385222\n",
      "Epochs: 1001, Loss: 0.6621137713242011\n",
      "Epochs: 1001, Loss: 0.6621054630896565\n",
      "Epochs: 1001, Loss: 0.6620972012827457\n",
      "Epochs: 1001, Loss: 0.6620889855547878\n",
      "Epochs: 1001, Loss: 0.662080815560518\n",
      "Epochs: 1001, Loss: 0.6620726909580469\n",
      "Epochs: 1001, Loss: 0.6620646114088174\n",
      "Epochs: 1001, Loss: 0.6620565765775634\n",
      "Epochs: 1001, Loss: 0.6620485861322697\n",
      "Epochs: 1001, Loss: 0.6620406397441311\n",
      "Epochs: 1001, Loss: 0.6620327370875129\n",
      "Epochs: 1001, Loss: 0.6620248778399125\n",
      "Epochs: 1001, Loss: 0.6620170616819202\n",
      "Epochs: 1001, Loss: 0.6620092882971825\n",
      "Epochs: 1001, Loss: 0.6620015573723639\n",
      "Epochs: 1001, Loss: 0.6619938685971104\n",
      "Epochs: 1001, Loss: 0.6619862216640136\n",
      "Epochs: 1001, Loss: 0.6619786162685746\n",
      "Epochs: 1001, Loss: 0.6619710521091706\n",
      "Epochs: 1001, Loss: 0.6619635288870175\n",
      "Epochs: 1001, Loss: 0.6619560463061379\n",
      "Epochs: 1001, Loss: 0.6619486040733276\n",
      "Epochs: 1001, Loss: 0.6619412018981213\n",
      "Epochs: 1001, Loss: 0.6619338394927607\n",
      "Epochs: 1001, Loss: 0.661926516572163\n",
      "Epochs: 1001, Loss: 0.6619192328538875\n",
      "Epochs: 1001, Loss: 0.6619119880581066\n",
      "Epochs: 1001, Loss: 0.6619047819075735\n",
      "Epochs: 1001, Loss: 0.6618976141275923\n",
      "Epochs: 1001, Loss: 0.6618904844459883\n",
      "Epochs: 1001, Loss: 0.6618833925930789\n",
      "Epochs: 1001, Loss: 0.6618763383016435\n",
      "Epochs: 1001, Loss: 0.661869321306896\n",
      "Epochs: 1001, Loss: 0.6618623413464564\n",
      "Epochs: 1001, Loss: 0.6618553981603222\n",
      "Epochs: 1001, Loss: 0.661848491490842\n",
      "Epochs: 1001, Loss: 0.6618416210826883\n",
      "Epochs: 1001, Loss: 0.66183478668283\n",
      "Epochs: 1001, Loss: 0.6618279880405076\n",
      "Epochs: 1001, Loss: 0.6618212249072059\n",
      "Epochs: 1001, Loss: 0.6618144970366293\n",
      "Epochs: 1001, Loss: 0.6618078041846768\n",
      "Epochs: 1001, Loss: 0.6618011461094164\n",
      "Epochs: 1001, Loss: 0.6617945225710608\n",
      "Epochs: 1001, Loss: 0.6617879333319442\n",
      "Epochs: 1001, Loss: 0.6617813781564972\n",
      "Epochs: 1001, Loss: 0.6617748568112245\n",
      "Epochs: 1001, Loss: 0.6617683690646806\n",
      "Epochs: 1001, Loss: 0.6617619146874476\n",
      "Epochs: 1001, Loss: 0.6617554934521132\n",
      "Epochs: 1001, Loss: 0.6617491051332473\n",
      "Epochs: 1001, Loss: 0.661742749507381\n",
      "Epochs: 1001, Loss: 0.6617364263529845\n",
      "Epochs: 1001, Loss: 0.6617301354504462\n",
      "Epochs: 1001, Loss: 0.6617238765820513\n",
      "Epochs: 1001, Loss: 0.6617176495319614\n",
      "Epochs: 1001, Loss: 0.6617114540861934\n",
      "Epochs: 1001, Loss: 0.6617052900325999\n",
      "Epochs: 1001, Loss: 0.6616991571608493\n",
      "Epochs: 1001, Loss: 0.661693055262406\n",
      "Epochs: 1001, Loss: 0.6616869841305104\n",
      "Epochs: 1001, Loss: 0.661680943560161\n",
      "Epochs: 1001, Loss: 0.6616749333480944\n",
      "Epochs: 1001, Loss: 0.6616689532927674\n",
      "Epochs: 1001, Loss: 0.6616630031943379\n",
      "Epochs: 1001, Loss: 0.6616570828546476\n",
      "Epochs: 1001, Loss: 0.6616511920772032\n",
      "Epochs: 1001, Loss: 0.6616453306671599\n",
      "Epochs: 1001, Loss: 0.6616394984313027\n",
      "Epochs: 1001, Loss: 0.6616336951780296\n",
      "Epochs: 1001, Loss: 0.6616279207173353\n",
      "Epochs: 1001, Loss: 0.6616221748607938\n",
      "Epochs: 1001, Loss: 0.6616164574215415\n",
      "Epochs: 1001, Loss: 0.6616107682142615\n",
      "Epochs: 1001, Loss: 0.6616051070551673\n",
      "Epochs: 1001, Loss: 0.661599473761987\n",
      "Epochs: 1001, Loss: 0.6615938681539472\n",
      "Epochs: 1001, Loss: 0.661588290051758\n",
      "Epochs: 1001, Loss: 0.6615827392775971\n",
      "Epochs: 1001, Loss: 0.6615772156550953\n",
      "Epochs: 1001, Loss: 0.6615717190093208\n",
      "Epochs: 1001, Loss: 0.6615662491667654\n",
      "Epochs: 1001, Loss: 0.6615608059553293\n",
      "Epochs: 1001, Loss: 0.6615553892043071\n",
      "Epochs: 1001, Loss: 0.6615499987443728\n",
      "Epochs: 1001, Loss: 0.6615446344075674\n",
      "Epochs: 1001, Loss: 0.6615392960272826\n",
      "Epochs: 1001, Loss: 0.6615339834382503\n",
      "Epochs: 1001, Loss: 0.6615286964765263\n",
      "Epochs: 1001, Loss: 0.6615234349794785\n",
      "Epochs: 1001, Loss: 0.6615181987857733\n",
      "Epochs: 1001, Loss: 0.6615129877353626\n",
      "Epochs: 1001, Loss: 0.661507801669471\n",
      "Epochs: 1001, Loss: 0.661502640430583\n",
      "Epochs: 1001, Loss: 0.6614975038624313\n",
      "Epochs: 1001, Loss: 0.661492391809983\n",
      "Epochs: 1001, Loss: 0.6614873041194282\n",
      "Epochs: 1001, Loss: 0.6614822406381684\n",
      "Epochs: 1001, Loss: 0.6614772012148037\n",
      "Epochs: 1001, Loss: 0.6614721856991211\n",
      "Epochs: 1001, Loss: 0.661467193942084\n",
      "Epochs: 1001, Loss: 0.6614622257958188\n",
      "Epochs: 1001, Loss: 0.6614572811136054\n",
      "Epochs: 1001, Loss: 0.661452359749865\n",
      "Epochs: 1001, Loss: 0.6614474615601492\n",
      "Epochs: 1001, Loss: 0.6614425864011291\n",
      "Epochs: 1001, Loss: 0.6614377341305842\n",
      "Epochs: 1001, Loss: 0.6614329046073919\n",
      "Epochs: 1001, Loss: 0.6614280976915181\n",
      "Epochs: 1001, Loss: 0.6614233132440043\n",
      "Epochs: 1001, Loss: 0.6614185511269594\n",
      "Epochs: 1001, Loss: 0.6614138112035481\n",
      "Epochs: 1001, Loss: 0.6614090933379827\n",
      "Epochs: 1001, Loss: 0.661404397395511\n",
      "Epochs: 1001, Loss: 0.6613997232424075\n",
      "Epochs: 1001, Loss: 0.661395070745964\n",
      "Epochs: 1001, Loss: 0.6613904397744793\n",
      "Epochs: 1001, Loss: 0.6613858301972502\n",
      "Epochs: 1001, Loss: 0.6613812418845613\n",
      "Epochs: 1001, Loss: 0.6613766747076772\n",
      "Epochs: 1001, Loss: 0.6613721285388324\n",
      "Epochs: 1001, Loss: 0.6613676032512217\n",
      "Epochs: 1001, Loss: 0.6613630987189921\n",
      "Epochs: 1001, Loss: 0.661358614817234\n",
      "Epochs: 1001, Loss: 0.6613541514219721\n",
      "Epochs: 1001, Loss: 0.6613497084101565\n",
      "Epochs: 1001, Loss: 0.6613452856596544\n",
      "Epochs: 1001, Loss: 0.6613408830492424\n",
      "Epochs: 1001, Loss: 0.6613365004585968\n",
      "Epochs: 1001, Loss: 0.661332137768286\n",
      "Epochs: 1001, Loss: 0.6613277948597625\n",
      "Epochs: 1001, Loss: 0.6613234716153548\n",
      "Epochs: 1001, Loss: 0.6613191679182583\n",
      "Epochs: 1001, Loss: 0.6613148836525297\n",
      "Epochs: 1001, Loss: 0.6613106187030768\n",
      "Epochs: 1001, Loss: 0.6613063729556523\n",
      "Epochs: 1001, Loss: 0.6613021462968454\n",
      "Epochs: 1001, Loss: 0.6612979386140747\n",
      "Epochs: 1001, Loss: 0.6612937497955805\n",
      "Epochs: 1001, Loss: 0.6612895797304171\n",
      "Epochs: 1001, Loss: 0.6612854283084466\n",
      "Epochs: 1001, Loss: 0.6612812954203301\n",
      "Epochs: 1001, Loss: 0.6612771809575222\n",
      "Epochs: 1001, Loss: 0.6612730848122625\n",
      "Epochs: 1001, Loss: 0.6612690068775695\n",
      "Epochs: 1001, Loss: 0.6612649470472332\n",
      "Epochs: 1001, Loss: 0.6612609052158087\n",
      "Epochs: 1001, Loss: 0.6612568812786096\n",
      "Epochs: 1001, Loss: 0.6612528751317\n",
      "Epochs: 1001, Loss: 0.6612488866718895\n",
      "Epochs: 1001, Loss: 0.6612449157967262\n",
      "Epochs: 1001, Loss: 0.6612409624044889\n",
      "Epochs: 1001, Loss: 0.6612370263941832\n",
      "Epochs: 1001, Loss: 0.6612331076655328\n",
      "Epochs: 1001, Loss: 0.6612292061189744\n",
      "Epochs: 1001, Loss: 0.6612253216556517\n",
      "Epochs: 1001, Loss: 0.6612214541774082\n",
      "Epochs: 1001, Loss: 0.6612176035867825\n",
      "Epochs: 1001, Loss: 0.6612137697870007\n",
      "Epochs: 1001, Loss: 0.6612099526819725\n",
      "Epochs: 1001, Loss: 0.6612061521762828\n",
      "Epochs: 1001, Loss: 0.6612023681751887\n",
      "Epochs: 1001, Loss: 0.6611986005846113\n",
      "Epochs: 1001, Loss: 0.6611948493111311\n",
      "Epochs: 1001, Loss: 0.6611911142619827\n",
      "Epochs: 1001, Loss: 0.6611873953450484\n",
      "Epochs: 1001, Loss: 0.6611836924688533\n",
      "Epochs: 1001, Loss: 0.6611800055425596\n",
      "Epochs: 1001, Loss: 0.661176334475961\n",
      "Epochs: 1001, Loss: 0.6611726791794774\n",
      "Epochs: 1001, Loss: 0.6611690395641506\n",
      "Epochs: 1001, Loss: 0.6611654155416374\n",
      "Epochs: 1001, Loss: 0.6611618070242054\n",
      "Epochs: 1001, Loss: 0.661158213924728\n",
      "Epochs: 1001, Loss: 0.6611546361566789\n",
      "Epochs: 1001, Loss: 0.6611510736341268\n",
      "Epochs: 1001, Loss: 0.6611475262717316\n",
      "Epochs: 1001, Loss: 0.6611439939847381\n",
      "Epochs: 1001, Loss: 0.6611404766889721\n",
      "Epochs: 1001, Loss: 0.6611369743008352\n",
      "Epochs: 1001, Loss: 0.6611334867372991\n",
      "Epochs: 1001, Loss: 0.6611300139159034\n",
      "Epochs: 1001, Loss: 0.6611265557547481\n",
      "Epochs: 1001, Loss: 0.6611231121724909\n",
      "Epochs: 1001, Loss: 0.661119683088341\n",
      "Epochs: 1001, Loss: 0.6611162684220566\n",
      "Epochs: 1001, Loss: 0.6611128680939385\n",
      "Epochs: 1001, Loss: 0.6611094820248268\n",
      "Epochs: 1001, Loss: 0.6611061101360958\n",
      "Epochs: 1001, Loss: 0.6611027523496498\n",
      "Epochs: 1001, Loss: 0.6610994085879197\n",
      "Epochs: 1001, Loss: 0.6610960787738572\n",
      "Epochs: 1001, Loss: 0.6610927628309318\n",
      "Epochs: 1001, Loss: 0.6610894606831254\n",
      "Epochs: 1001, Loss: 0.6610861722549299\n",
      "Epochs: 1001, Loss: 0.6610828974713413\n",
      "Epochs: 1001, Loss: 0.6610796362578566\n",
      "Epochs: 1001, Loss: 0.6610763885404694\n",
      "Epochs: 1001, Loss: 0.6610731542456666\n",
      "Epochs: 1001, Loss: 0.6610699333004233\n",
      "Epochs: 1001, Loss: 0.6610667256321997\n",
      "Epochs: 1001, Loss: 0.6610635311689372\n",
      "Epochs: 1001, Loss: 0.6610603498390543\n",
      "Epochs: 1001, Loss: 0.6610571815714427\n",
      "Epochs: 1001, Loss: 0.6610540262954636\n",
      "Epochs: 1001, Loss: 0.6610508839409448\n",
      "Epochs: 1001, Loss: 0.6610477544381755\n",
      "Epochs: 1001, Loss: 0.661044637717904\n",
      "Epochs: 1001, Loss: 0.6610415337113329\n",
      "Epochs: 1001, Loss: 0.6610384423501166\n",
      "Epochs: 1001, Loss: 0.6610353635663572\n",
      "Epochs: 1001, Loss: 0.661032297292601\n",
      "Epochs: 1001, Loss: 0.6610292434618351\n",
      "Epochs: 1001, Loss: 0.6610262020074836\n",
      "Epochs: 1001, Loss: 0.6610231728634051\n",
      "Epochs: 1001, Loss: 0.6610201559638876\n",
      "Epochs: 1001, Loss: 0.661017151243648\n",
      "Epochs: 1001, Loss: 0.661014158637825\n",
      "Epochs: 1001, Loss: 0.6610111780819792\n",
      "Epochs: 1001, Loss: 0.661008209512088\n",
      "Epochs: 1001, Loss: 0.6610052528645427\n",
      "Epochs: 1001, Loss: 0.6610023080761456\n",
      "Epochs: 1001, Loss: 0.6609993750841066\n",
      "Epochs: 1001, Loss: 0.6609964538260399\n",
      "Epochs: 1001, Loss: 0.6609935442399615\n",
      "Epochs: 1001, Loss: 0.660990646264285\n",
      "Epochs: 1001, Loss: 0.6609877598378201\n",
      "Epochs: 1001, Loss: 0.6609848848997678\n",
      "Epochs: 1001, Loss: 0.6609820213897191\n",
      "Epochs: 1001, Loss: 0.6609791692476507\n",
      "Epochs: 1001, Loss: 0.6609763284139227\n",
      "Epochs: 1001, Loss: 0.6609734988292757\n",
      "Epochs: 1001, Loss: 0.6609706804348272\n",
      "Epochs: 1001, Loss: 0.6609678731720706\n",
      "Epochs: 1001, Loss: 0.6609650769828695\n",
      "Epochs: 1001, Loss: 0.6609622918094575\n",
      "Epochs: 1001, Loss: 0.6609595175944336\n",
      "Epochs: 1001, Loss: 0.6609567542807608\n",
      "Epochs: 1001, Loss: 0.6609540018117626\n",
      "Epochs: 1001, Loss: 0.6609512601311197\n",
      "Epochs: 1001, Loss: 0.6609485291828692\n",
      "Epochs: 1001, Loss: 0.6609458089114002\n",
      "Epochs: 1001, Loss: 0.6609430992614513\n",
      "Epochs: 1001, Loss: 0.6609404001781093\n",
      "Epochs: 1001, Loss: 0.6609377116068049\n",
      "Epochs: 1001, Loss: 0.6609350334933115\n",
      "Epochs: 1001, Loss: 0.6609323657837415\n",
      "Epochs: 1001, Loss: 0.6609297084245451\n",
      "Epochs: 1001, Loss: 0.6609270613625065\n",
      "Epochs: 1001, Loss: 0.660924424544742\n",
      "Epochs: 1001, Loss: 0.6609217979186978\n",
      "Epochs: 1001, Loss: 0.6609191814321467\n",
      "Epochs: 1001, Loss: 0.6609165750331871\n",
      "Epochs: 1001, Loss: 0.6609139786702389\n",
      "Epochs: 1001, Loss: 0.6609113922920429\n",
      "Epochs: 1001, Loss: 0.6609088158476564\n",
      "Epochs: 1001, Loss: 0.6609062492864528\n",
      "Epochs: 1001, Loss: 0.6609036925581184\n",
      "Epochs: 1001, Loss: 0.6609011456126498\n",
      "Epochs: 1001, Loss: 0.6608986084003521\n",
      "Epochs: 1001, Loss: 0.6608960808718372\n",
      "Epochs: 1001, Loss: 0.6608935629780197\n",
      "Epochs: 1001, Loss: 0.6608910546701172\n",
      "Epochs: 1001, Loss: 0.6608885558996457\n",
      "Epochs: 1001, Loss: 0.6608860666184194\n",
      "Epochs: 1001, Loss: 0.6608835867785471\n",
      "Epochs: 1001, Loss: 0.660881116332431\n",
      "Epochs: 1001, Loss: 0.6608786552327636\n",
      "Epochs: 1001, Loss: 0.6608762034325271\n",
      "Epochs: 1001, Loss: 0.6608737608849896\n",
      "Epochs: 1001, Loss: 0.6608713275437045\n",
      "Epochs: 1001, Loss: 0.6608689033625071\n",
      "Epochs: 1001, Loss: 0.6608664882955142\n",
      "Epochs: 1001, Loss: 0.6608640822971201\n",
      "Epochs: 1001, Loss: 0.6608616853219966\n",
      "Epochs: 1001, Loss: 0.6608592973250892\n",
      "Epochs: 1001, Loss: 0.6608569182616166\n",
      "Epochs: 1001, Loss: 0.6608545480870682\n",
      "Epochs: 1001, Loss: 0.6608521867572016\n",
      "Epochs: 1001, Loss: 0.660849834228042\n",
      "Epochs: 1001, Loss: 0.6608474904558788\n",
      "Epochs: 1001, Loss: 0.6608451553972643\n",
      "Epochs: 1001, Loss: 0.6608428290090128\n",
      "Epochs: 1001, Loss: 0.6608405112481975\n",
      "Epochs: 1001, Loss: 0.6608382020721488\n",
      "Epochs: 1001, Loss: 0.660835901438453\n",
      "Epochs: 1001, Loss: 0.6608336093049504\n",
      "Epochs: 1001, Loss: 0.6608313256297328\n",
      "Epochs: 1001, Loss: 0.6608290503711431\n",
      "Epochs: 1001, Loss: 0.6608267834877719\n",
      "Epochs: 1001, Loss: 0.6608245249384572\n",
      "Epochs: 1001, Loss: 0.6608222746822816\n",
      "Epochs: 1001, Loss: 0.6608200326785714\n",
      "Epochs: 1001, Loss: 0.660817798886894\n",
      "Epochs: 1001, Loss: 0.6608155732670571\n",
      "Epochs: 1001, Loss: 0.6608133557791066\n",
      "Epochs: 1001, Loss: 0.6608111463833249\n",
      "Epochs: 1001, Loss: 0.6608089450402298\n",
      "Epochs: 1001, Loss: 0.6608067517105713\n",
      "Epochs: 1001, Loss: 0.6608045663553322\n",
      "Epochs: 1001, Loss: 0.6608023889357248\n",
      "Epochs: 1001, Loss: 0.6608002194131902\n",
      "Epochs: 1001, Loss: 0.660798057749396\n",
      "Epochs: 1001, Loss: 0.6607959039062353\n",
      "Epochs: 1001, Loss: 0.6607937578458251\n",
      "Epochs: 1001, Loss: 0.6607916195305041\n",
      "Epochs: 1001, Loss: 0.6607894889228327\n",
      "Epochs: 1001, Loss: 0.6607873659855892\n",
      "Epochs: 1001, Loss: 0.6607852506817706\n",
      "Epochs: 1001, Loss: 0.6607831429745896\n",
      "Epochs: 1001, Loss: 0.6607810428274736\n",
      "Epochs: 1001, Loss: 0.6607789502040632\n",
      "Epochs: 1001, Loss: 0.6607768650682107\n",
      "Epochs: 1001, Loss: 0.6607747873839788\n",
      "Epochs: 1001, Loss: 0.660772717115639\n",
      "Epochs: 1001, Loss: 0.6607706542276707\n",
      "Epochs: 1001, Loss: 0.6607685986847583\n",
      "Epochs: 1001, Loss: 0.6607665504517916\n",
      "Epochs: 1001, Loss: 0.6607645094938637\n",
      "Epochs: 1001, Loss: 0.6607624757762689\n",
      "Epochs: 1001, Loss: 0.6607604492645025\n",
      "Epochs: 1001, Loss: 0.6607584299242589\n",
      "Epochs: 1001, Loss: 0.6607564177214299\n",
      "Epochs: 1001, Loss: 0.6607544126221043\n",
      "Epochs: 1001, Loss: 0.6607524145925654\n",
      "Epochs: 1001, Loss: 0.6607504235992907\n",
      "Epochs: 1001, Loss: 0.6607484396089498\n",
      "Epochs: 1001, Loss: 0.6607464625884039\n",
      "Epochs: 1001, Loss: 0.6607444925047044\n",
      "Epochs: 1001, Loss: 0.6607425293250899\n",
      "Epochs: 1001, Loss: 0.6607405730169882\n",
      "Epochs: 1001, Loss: 0.6607386235480116\n",
      "Epochs: 1001, Loss: 0.6607366808859586\n",
      "Epochs: 1001, Loss: 0.6607347449988105\n",
      "Epochs: 1001, Loss: 0.660732815854731\n",
      "Epochs: 1001, Loss: 0.6607308934220653\n",
      "Epochs: 1001, Loss: 0.6607289776693384\n",
      "Epochs: 1001, Loss: 0.6607270685652542\n",
      "Epochs: 1001, Loss: 0.660725166078694\n",
      "Epochs: 1001, Loss: 0.6607232701787156\n",
      "Epochs: 1001, Loss: 0.6607213808345519\n",
      "Epochs: 1001, Loss: 0.6607194980156101\n",
      "Epochs: 1001, Loss: 0.6607176216914703\n",
      "Epochs: 1001, Loss: 0.6607157518318839\n",
      "Epochs: 1001, Loss: 0.6607138884067739\n",
      "Epochs: 1001, Loss: 0.6607120313862317\n",
      "Epochs: 1001, Loss: 0.6607101807405177\n",
      "Epochs: 1001, Loss: 0.6607083364400596\n",
      "Epochs: 1001, Loss: 0.660706498455451\n",
      "Epochs: 1001, Loss: 0.6607046667574512\n",
      "Epochs: 1001, Loss: 0.6607028413169825\n",
      "Epochs: 1001, Loss: 0.6607010221051307\n",
      "Epochs: 1001, Loss: 0.6606992090931437\n",
      "Epochs: 1001, Loss: 0.66069740225243\n",
      "Epochs: 1001, Loss: 0.6606956015545574\n",
      "Epochs: 1001, Loss: 0.6606938069712531\n",
      "Epochs: 1001, Loss: 0.6606920184744014\n",
      "Epochs: 1001, Loss: 0.6606902360360438\n",
      "Epochs: 1001, Loss: 0.6606884596283769\n",
      "Epochs: 1001, Loss: 0.6606866892237521\n",
      "Epochs: 1001, Loss: 0.6606849247946744\n",
      "Epochs: 1001, Loss: 0.6606831663138014\n",
      "Epochs: 1001, Loss: 0.6606814137539425\n",
      "Epochs: 1001, Loss: 0.6606796670880573\n",
      "Epochs: 1001, Loss: 0.6606779262892557\n",
      "Epochs: 1001, Loss: 0.6606761913307954\n",
      "Epochs: 1001, Loss: 0.6606744621860825\n",
      "Epochs: 1001, Loss: 0.6606727388286698\n",
      "Epochs: 1001, Loss: 0.660671021232256\n",
      "Epochs: 1001, Loss: 0.6606693093706841\n",
      "Epochs: 1001, Loss: 0.6606676032179414\n",
      "Epochs: 1001, Loss: 0.6606659027481588\n",
      "Epochs: 1001, Loss: 0.6606642079356083\n",
      "Epochs: 1001, Loss: 0.660662518754704\n",
      "Epochs: 1001, Loss: 0.6606608351799996\n",
      "Epochs: 1001, Loss: 0.6606591571861886\n",
      "Epochs: 1001, Loss: 0.6606574847481028\n",
      "Epochs: 1001, Loss: 0.6606558178407118\n",
      "Epochs: 1001, Loss: 0.6606541564391218\n",
      "Epochs: 1001, Loss: 0.6606525005185749\n",
      "Epochs: 1001, Loss: 0.6606508500544481\n",
      "Epochs: 1001, Loss: 0.6606492050222528\n",
      "Epochs: 1001, Loss: 0.6606475653976335\n",
      "Epochs: 1001, Loss: 0.6606459311563672\n",
      "Epochs: 1001, Loss: 0.6606443022743623\n",
      "Epochs: 1001, Loss: 0.6606426787276583\n",
      "Epochs: 1001, Loss: 0.6606410604924244\n",
      "Epochs: 1001, Loss: 0.660639447544959\n",
      "Epochs: 1001, Loss: 0.6606378398616888\n",
      "Epochs: 1001, Loss: 0.6606362374191679\n",
      "Epochs: 1001, Loss: 0.6606346401940774\n",
      "Epochs: 1001, Loss: 0.6606330481632238\n",
      "Epochs: 1001, Loss: 0.6606314613035388\n",
      "Epochs: 1001, Loss: 0.6606298795920789\n",
      "Epochs: 1001, Loss: 0.6606283030060235\n",
      "Epochs: 1001, Loss: 0.6606267315226749\n",
      "Epochs: 1001, Loss: 0.660625165119458\n",
      "Epochs: 1001, Loss: 0.6606236037739177\n",
      "Epochs: 1001, Loss: 0.6606220474637206\n",
      "Epochs: 1001, Loss: 0.660620496166652\n",
      "Epochs: 1001, Loss: 0.6606189498606168\n",
      "Epochs: 1001, Loss: 0.6606174085236379\n",
      "Epochs: 1001, Loss: 0.6606158721338555\n",
      "Epochs: 1001, Loss: 0.6606143406695271\n",
      "Epochs: 1001, Loss: 0.6606128141090254\n",
      "Epochs: 1001, Loss: 0.6606112924308389\n",
      "Epochs: 1001, Loss: 0.6606097756135705\n",
      "Epochs: 1001, Loss: 0.6606082636359372\n",
      "Epochs: 1001, Loss: 0.6606067564767687\n",
      "Epochs: 1001, Loss: 0.6606052541150075\n",
      "Epochs: 1001, Loss: 0.6606037565297082\n",
      "Epochs: 1001, Loss: 0.6606022637000355\n",
      "Epochs: 1001, Loss: 0.6606007756052651\n",
      "Epochs: 1001, Loss: 0.6605992922247828\n",
      "Epochs: 1001, Loss: 0.6605978135380822\n",
      "Epochs: 1001, Loss: 0.6605963395247665\n",
      "Epochs: 1001, Loss: 0.6605948701645455\n",
      "Epochs: 1001, Loss: 0.6605934054372373\n",
      "Epochs: 1001, Loss: 0.6605919453227652\n",
      "Epochs: 1001, Loss: 0.6605904898011588\n",
      "Epochs: 1001, Loss: 0.6605890388525522\n",
      "Epochs: 1001, Loss: 0.6605875924571846\n",
      "Epochs: 1001, Loss: 0.6605861505953986\n",
      "Epochs: 1001, Loss: 0.66058471324764\n",
      "Epochs: 1001, Loss: 0.660583280394457\n",
      "Epochs: 1001, Loss: 0.6605818520165\n",
      "Epochs: 1001, Loss: 0.6605804280945202\n",
      "Epochs: 1001, Loss: 0.6605790086093698\n",
      "Epochs: 1001, Loss: 0.660577593542001\n",
      "Epochs: 1001, Loss: 0.660576182873465\n",
      "Epochs: 1001, Loss: 0.6605747765849123\n",
      "Epochs: 1001, Loss: 0.6605733746575916\n",
      "Epochs: 1001, Loss: 0.6605719770728485\n",
      "Epochs: 1001, Loss: 0.6605705838121269\n",
      "Epochs: 1001, Loss: 0.6605691948569659\n",
      "Epochs: 1001, Loss: 0.6605678101890013\n",
      "Epochs: 1001, Loss: 0.6605664297899637\n",
      "Epochs: 1001, Loss: 0.6605650536416782\n",
      "Epochs: 1001, Loss: 0.6605636817260648\n",
      "Epochs: 1001, Loss: 0.6605623140251364\n",
      "Epochs: 1001, Loss: 0.6605609505209991\n",
      "Epochs: 1001, Loss: 0.6605595911958515\n",
      "Epochs: 1001, Loss: 0.660558236031984\n",
      "Epochs: 1001, Loss: 0.660556885011778\n",
      "Epochs: 1001, Loss: 0.6605555381177065\n",
      "Epochs: 1001, Loss: 0.660554195332332\n",
      "Epochs: 1001, Loss: 0.6605528566383068\n",
      "Epochs: 1001, Loss: 0.6605515220183723\n",
      "Epochs: 1001, Loss: 0.6605501914553592\n",
      "Epochs: 1001, Loss: 0.6605488649321853\n",
      "Epochs: 1001, Loss: 0.6605475424318566\n",
      "Epochs: 1001, Loss: 0.6605462239374662\n",
      "Epochs: 1001, Loss: 0.6605449094321929\n",
      "Epochs: 1001, Loss: 0.6605435988993026\n",
      "Epochs: 1001, Loss: 0.6605422923221457\n",
      "Epochs: 1001, Loss: 0.6605409896841585\n",
      "Epochs: 1001, Loss: 0.660539690968861\n",
      "Epochs: 1001, Loss: 0.6605383961598575\n",
      "Epochs: 1001, Loss: 0.6605371052408359\n",
      "Epochs: 1001, Loss: 0.6605358181955668\n",
      "Epochs: 1001, Loss: 0.6605345350079038\n",
      "Epochs: 1001, Loss: 0.6605332556617817\n",
      "Epochs: 1001, Loss: 0.6605319801412174\n",
      "Epochs: 1001, Loss: 0.6605307084303087\n",
      "Epochs: 1001, Loss: 0.6605294405132338\n",
      "Epochs: 1001, Loss: 0.6605281763742513\n",
      "Epochs: 1001, Loss: 0.6605269159976993\n",
      "Epochs: 1001, Loss: 0.6605256593679946\n",
      "Epochs: 1001, Loss: 0.6605244064696332\n",
      "Epochs: 1001, Loss: 0.6605231572871892\n",
      "Epochs: 1001, Loss: 0.6605219118053143\n",
      "Epochs: 1001, Loss: 0.6605206700087379\n",
      "Epochs: 1001, Loss: 0.6605194318822654\n",
      "Epochs: 1001, Loss: 0.6605181974107793\n",
      "Epochs: 1001, Loss: 0.6605169665792382\n",
      "Epochs: 1001, Loss: 0.6605157393726754\n",
      "Epochs: 1001, Loss: 0.6605145157761999\n",
      "Epochs: 1001, Loss: 0.6605132957749953\n",
      "Epochs: 1001, Loss: 0.6605120793543187\n",
      "Epochs: 1001, Loss: 0.6605108664995019\n",
      "Epochs: 1001, Loss: 0.6605096571959495\n",
      "Epochs: 1001, Loss: 0.660508451429139\n",
      "Epochs: 1001, Loss: 0.6605072491846206\n",
      "Epochs: 1001, Loss: 0.660506050448016\n",
      "Epochs: 1001, Loss: 0.6605048552050196\n",
      "Epochs: 1001, Loss: 0.6605036634413957\n",
      "Epochs: 1001, Loss: 0.6605024751429803\n",
      "Epochs: 1001, Loss: 0.6605012902956796\n",
      "Epochs: 1001, Loss: 0.6605001088854696\n",
      "Epochs: 1001, Loss: 0.6604989308983962\n",
      "Epochs: 1001, Loss: 0.660497756320574\n",
      "Epochs: 1001, Loss: 0.6604965851381868\n",
      "Epochs: 1001, Loss: 0.6604954173374866\n",
      "Epochs: 1001, Loss: 0.6604942529047934\n",
      "Epochs: 1001, Loss: 0.660493091826495\n",
      "Epochs: 1001, Loss: 0.6604919340890458\n",
      "Epochs: 1001, Loss: 0.660490779678968\n",
      "Epochs: 1001, Loss: 0.6604896285828494\n",
      "Epochs: 1001, Loss: 0.660488480787344\n",
      "Epochs: 1001, Loss: 0.6604873362791718\n",
      "Epochs: 1001, Loss: 0.6604861950451181\n",
      "Epochs: 1001, Loss: 0.6604850570720323\n",
      "Epochs: 1001, Loss: 0.6604839223468297\n",
      "Epochs: 1001, Loss: 0.6604827908564889\n",
      "Epochs: 1001, Loss: 0.6604816625880523\n",
      "Epochs: 1001, Loss: 0.6604805375286261\n",
      "Epochs: 1001, Loss: 0.6604794156653796\n",
      "Epochs: 1001, Loss: 0.6604782969855444\n",
      "Epochs: 1001, Loss: 0.6604771814764147\n",
      "Epochs: 1001, Loss: 0.6604760691253471\n",
      "Epochs: 1001, Loss: 0.6604749599197594\n",
      "Epochs: 1001, Loss: 0.6604738538471305\n",
      "Epochs: 1001, Loss: 0.6604727508950016\n",
      "Epochs: 1001, Loss: 0.6604716510509727\n",
      "Epochs: 1001, Loss: 0.6604705543027052\n",
      "Epochs: 1001, Loss: 0.6604694606379201\n",
      "Epochs: 1001, Loss: 0.6604683700443985\n",
      "Epochs: 1001, Loss: 0.6604672825099799\n",
      "Epochs: 1001, Loss: 0.660466198022563\n",
      "Epochs: 1001, Loss: 0.6604651165701059\n",
      "Epochs: 1001, Loss: 0.660464038140624\n",
      "Epochs: 1001, Loss: 0.6604629627221912\n",
      "Epochs: 1001, Loss: 0.6604618903029383\n",
      "Epochs: 1001, Loss: 0.6604608208710542\n",
      "Epochs: 1001, Loss: 0.6604597544147846\n",
      "Epochs: 1001, Loss: 0.6604586909224315\n",
      "Epochs: 1001, Loss: 0.6604576303823535\n",
      "Epochs: 1001, Loss: 0.6604565727829651\n",
      "Epochs: 1001, Loss: 0.6604555181127363\n",
      "Epochs: 1001, Loss: 0.6604544663601931\n",
      "Epochs: 1001, Loss: 0.6604534175139162\n",
      "Epochs: 1001, Loss: 0.6604523715625406\n",
      "Epochs: 1001, Loss: 0.6604513284947569\n",
      "Epochs: 1001, Loss: 0.6604502882993083\n",
      "Epochs: 1001, Loss: 0.6604492509649936\n",
      "Epochs: 1001, Loss: 0.6604482164806639\n",
      "Epochs: 1001, Loss: 0.660447184835224\n",
      "Epochs: 1001, Loss: 0.6604461560176316\n",
      "Epochs: 1001, Loss: 0.6604451300168971\n",
      "Epochs: 1001, Loss: 0.6604441068220833\n",
      "Epochs: 1001, Loss: 0.660443086422305\n",
      "Epochs: 1001, Loss: 0.6604420688067292\n",
      "Epochs: 1001, Loss: 0.6604410539645733\n",
      "Epochs: 1001, Loss: 0.6604400418851074\n",
      "Epochs: 1001, Loss: 0.6604390325576512\n",
      "Epochs: 1001, Loss: 0.6604380259715763\n",
      "Epochs: 1001, Loss: 0.6604370221163035\n",
      "Epochs: 1001, Loss: 0.6604360209813047\n",
      "Epochs: 1001, Loss: 0.660435022556101\n",
      "Epochs: 1001, Loss: 0.6604340268302632\n",
      "Epochs: 1001, Loss: 0.6604330337934112\n",
      "Epochs: 1001, Loss: 0.6604320434352148\n",
      "Epochs: 1001, Loss: 0.6604310557453912\n",
      "Epochs: 1001, Loss: 0.6604300707137069\n",
      "Epochs: 1001, Loss: 0.6604290883299764\n",
      "Epochs: 1001, Loss: 0.6604281085840621\n",
      "Epochs: 1001, Loss: 0.6604271314658744\n",
      "Epochs: 1001, Loss: 0.6604261569653705\n",
      "Epochs: 1001, Loss: 0.6604251850725553\n",
      "Epochs: 1001, Loss: 0.6604242157774807\n",
      "Epochs: 1001, Loss: 0.6604232490702442\n",
      "Epochs: 1001, Loss: 0.6604222849409911\n",
      "Epochs: 1001, Loss: 0.6604213233799118\n",
      "Epochs: 1001, Loss: 0.6604203643772429\n",
      "Epochs: 1001, Loss: 0.6604194079232671\n",
      "Epochs: 1001, Loss: 0.6604184540083117\n",
      "Epochs: 1001, Loss: 0.6604175026227495\n",
      "Epochs: 1001, Loss: 0.6604165537569981\n",
      "Epochs: 1001, Loss: 0.6604156074015203\n",
      "Epochs: 1001, Loss: 0.6604146635468223\n",
      "Epochs: 1001, Loss: 0.6604137221834553\n",
      "Epochs: 1001, Loss: 0.6604127833020139\n",
      "Epochs: 1001, Loss: 0.6604118468931368\n",
      "Epochs: 1001, Loss: 0.660410912947506\n",
      "Epochs: 1001, Loss: 0.6604099814558464\n",
      "Epochs: 1001, Loss: 0.6604090524089266\n",
      "Epochs: 1001, Loss: 0.660408125797557\n",
      "Epochs: 1001, Loss: 0.6604072016125913\n",
      "Epochs: 1001, Loss: 0.6604062798449252\n",
      "Epochs: 1001, Loss: 0.6604053604854967\n",
      "Epochs: 1001, Loss: 0.6604044435252849\n",
      "Epochs: 1001, Loss: 0.6604035289553114\n",
      "Epochs: 1001, Loss: 0.6604026167666388\n",
      "Epochs: 1001, Loss: 0.6604017069503707\n",
      "Epochs: 1001, Loss: 0.6604007994976522\n",
      "Epochs: 1001, Loss: 0.6603998943996684\n",
      "Epochs: 1001, Loss: 0.6603989916476452\n",
      "Epochs: 1001, Loss: 0.660398091232849\n",
      "Epochs: 1001, Loss: 0.6603971931465863\n",
      "Epochs: 1001, Loss: 0.6603962973802031\n",
      "Epochs: 1001, Loss: 0.6603954039250852\n",
      "Epochs: 1001, Loss: 0.6603945127726579\n",
      "Epochs: 1001, Loss: 0.6603936239143858\n",
      "Epochs: 1001, Loss: 0.6603927373417723\n",
      "Epochs: 1001, Loss: 0.6603918530463598\n",
      "Epochs: 1001, Loss: 0.660390971019729\n",
      "Epochs: 1001, Loss: 0.6603900912534995\n",
      "Epochs: 1001, Loss: 0.6603892137393286\n",
      "Epochs: 1001, Loss: 0.660388338468912\n",
      "Epochs: 1001, Loss: 0.6603874654339829\n",
      "Epochs: 1001, Loss: 0.6603865946263122\n",
      "Epochs: 1001, Loss: 0.660385726037708\n",
      "Epochs: 1001, Loss: 0.660384859660016\n",
      "Epochs: 1001, Loss: 0.6603839954851185\n",
      "Epochs: 1001, Loss: 0.6603831335049349\n",
      "Epochs: 1001, Loss: 0.6603822737114209\n",
      "Epochs: 1001, Loss: 0.6603814160965691\n",
      "Epochs: 1001, Loss: 0.6603805606524075\n",
      "Epochs: 1001, Loss: 0.6603797073710008\n",
      "Epochs: 1001, Loss: 0.6603788562444497\n",
      "Epochs: 1001, Loss: 0.6603780072648897\n",
      "Epochs: 1001, Loss: 0.6603771604244925\n",
      "Epochs: 1001, Loss: 0.6603763157154651\n",
      "Epochs: 1001, Loss: 0.6603754731300489\n",
      "Epochs: 1001, Loss: 0.660374632660521\n",
      "Epochs: 1001, Loss: 0.6603737942991925\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = MySimpleNN(X_test_tensor)\n",
    "model.weights\n",
    "\n",
    "## define loop\n",
    "for epoch in range(epochs):\n",
    "    ## forward pass\n",
    "    y_pred = model.forward(X_test_tensor)\n",
    "\n",
    "    ## loss calculate\n",
    "    loss = model.loss_function(y_pred, y_test_tensor)\n",
    "\n",
    "    ## backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    ## paramters update\n",
    "    with torch.no_grad():\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bais -= learning_rate * model.bais.grad\n",
    "\n",
    "    ## zero gradients\n",
    "    model.weights.grad.zero_()\n",
    "    model.bais.grad.zero_()\n",
    "\n",
    "    ## print loss in each epochs\n",
    "    print(f'Epochs: {epochs + 1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2413],\n",
       "        [-0.0847],\n",
       "        [ 0.4987],\n",
       "        [-0.0614],\n",
       "        [-0.0301],\n",
       "        [-0.1477],\n",
       "        [-0.1466],\n",
       "        [ 0.0360],\n",
       "        [ 0.0739],\n",
       "        [ 0.1480],\n",
       "        [-0.2449],\n",
       "        [-0.0048],\n",
       "        [ 0.0353],\n",
       "        [ 0.3198],\n",
       "        [ 0.0391],\n",
       "        [-0.1346],\n",
       "        [ 0.0171],\n",
       "        [-0.0198],\n",
       "        [ 0.0240],\n",
       "        [ 0.0531],\n",
       "        [-0.3388],\n",
       "        [ 0.0957],\n",
       "        [-0.0940],\n",
       "        [ 0.1956],\n",
       "        [-0.0348],\n",
       "        [ 0.1849],\n",
       "        [ 0.0816],\n",
       "        [ 0.1263],\n",
       "        [-0.0908],\n",
       "        [-0.1251]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6228070259094238\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model.forward(X_test_tensor)\n",
    "    y_pred = (y_pred > 0.5).float()\n",
    "    accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
