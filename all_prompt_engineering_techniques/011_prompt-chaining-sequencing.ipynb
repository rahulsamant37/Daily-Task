{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4a8b23",
   "metadata": {},
   "source": [
    "# Prompt Chaining and Sequencing Tutorial\n",
    "### Overview\n",
    "This tutorial explores the concepts of prompt chaining and sequencing in the context of working with large language models. We'll use Gemini models and the LangChain library to demonstrate how to connect multiple prompts and build logical flows for more complex AI-driven tasks.\n",
    "\n",
    "### Motivation\n",
    "As AI applications become more sophisticated, there's often a need to break down complex tasks into smaller, manageable steps. Prompt chaining and sequencing allow us to guide language models through a series of interrelated prompts, enabling more structured and controlled outputs. This approach is particularly useful for tasks that require multiple stages of processing or decision-making.\n",
    "\n",
    "### Key Components\n",
    "1. Basic Prompt Chaining: Connecting the output of one prompt to the input of another.\n",
    "2. Sequential Prompting: Creating a logical flow of prompts to guide the AI through a multi-step process.\n",
    "3. Dynamic Prompt Generation: Using the output of one prompt to dynamically generate the next prompt.\n",
    "4. Error Handling and Validation: Implementing checks and balances within the prompt chain.\n",
    "### Method Details\n",
    "We'll start by setting up our environment with the necessary libraries. Then, we'll explore basic prompt chaining by connecting two simple prompts. We'll move on to more complex sequential prompting, where we'll guide the AI through a multi-step analysis process. Next, we'll demonstrate how to dynamically generate prompts based on previous outputs. Finally, we'll implement error handling and validation techniques to make our prompt chains more robust.\n",
    "\n",
    "Throughout the tutorial, we'll use practical examples to illustrate these concepts, such as a multi-step text analysis task and a dynamic question-answering system.\n",
    "\n",
    "### Conclusion\n",
    "By the end of this tutorial, you'll have a solid understanding of how to implement prompt chaining and sequencing in your AI applications. These techniques will enable you to tackle more complex tasks, improve the coherence and relevance of AI-generated content, and create more interactive and dynamic AI-driven experiences.\n",
    "\n",
    "### Setup\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84eabe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load enviroment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Google API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86d17d",
   "metadata": {},
   "source": [
    "### Basic Prompt Chaining\n",
    "Let's start with a simple example of prompt chaining. We'll create two prompts: one to generate a short story, and another to summarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430f6ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: The chronometer flickered, displaying a date centuries in the past. Captain Eva cursed, realizing the temporal jump had gone awry, stranding them in a primitive Earth where dinosaurs roamed.  Suddenly, a colossal Tyrannosaurus Rex emerged from the jungle, its eyes fixated on their time-traveling vessel, mistaking it for a giant, metal egg. Eva grabbed her laser rifle, knowing their mission to preserve history had just become a desperate fight for survival.\n",
      "\n",
      "Summary: A disastrous temporal jump strands Captain Eva and her crew in the dinosaur age, forcing them to fight for survival against a Tyrannosaurus Rex who mistakes their time machine for an egg.\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates\n",
    "story_prompt = PromptTemplate(\n",
    "    input_variables=[\"genre\"],\n",
    "    template=\"Write a short {genre} story in 3-4 sentences.\"\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"story\"],\n",
    "    template=\"Summarize the following story in one sentence:\\n{story}\"\n",
    ")\n",
    "\n",
    "# Chain the prompts\n",
    "def story_chain(genre):\n",
    "    \"\"\"Generate a story and its summary based on a given genre.\n",
    "\n",
    "    Args:\n",
    "        genre (str): The genre of the story to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated story and its summary.\n",
    "    \"\"\"\n",
    "    story = (story_prompt | llm).invoke({\"genre\": genre}).content\n",
    "    summary = (summary_prompt | llm).invoke({\"story\": story}).content\n",
    "    return story, summary\n",
    "\n",
    "# Test the chain\n",
    "genre = \"science fiction\"\n",
    "story, summary = story_chain(genre)\n",
    "print(f\"Story: {story}\\n\\nSummary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa482e",
   "metadata": {},
   "source": [
    "### Sequential Prompting\n",
    "Now, let's create a more complex sequence of prompts for a multi-step analysis task. We'll analyze a given text for its main theme, tone, and key takeaways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c979f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme: The main theme is the **promise and peril of artificial intelligence, and the need for careful and ethical development.**\n",
      "\n",
      "Tone: The overall tone of the text is **cautious and balanced.**\n",
      "\n",
      "Here's why:\n",
      "\n",
      "*   **Excitement and Concern:** The text acknowledges both the positive (\"excitement,\" \"revolutionize industries,\" \"improve our daily lives\") and negative (\"concern,\" \"ethical questions,\" \"privacy, job displacement, and the potential for misuse\") aspects of AI. This creates a balanced perspective.\n",
      "*   **\"Caution and Foresight\":** The concluding sentence explicitly calls for a cautious approach, further reinforcing the overall tone.\n",
      "*   **Neutral Language:** While acknowledging potential risks, the text avoids overly alarmist or hyperbolic language. It presents the issues in a reasoned and measured way.\n",
      "*   **Focus on Responsibility:** The emphasis on \"ensuring that its benefits are maximized while its risks are minimized\" highlights a sense of responsibility and the need for careful consideration.\n",
      "\n",
      "In summary, the text isn't overly optimistic or pessimistic. Instead, it adopts a thoughtful and considered tone, urging careful navigation of the opportunities and challenges presented by AI.\n",
      "\n",
      "Takeaways: Based on the text and the provided analysis, here are the key takeaways:\n",
      "\n",
      "*   **AI is rapidly advancing and has the potential to be transformative:** It can revolutionize industries and improve daily life.\n",
      "*   **AI also presents significant risks and ethical concerns:** These include privacy violations, job displacement, and the potential for misuse.\n",
      "*   **A cautious and thoughtful approach is crucial:** AI development should be approached with foresight to maximize benefits and minimize risks.\n",
      "*   **Responsibility is key:** We must ensure AI is developed and used in a way that benefits humanity while mitigating potential harms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates for each step\n",
    "theme_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Identify the main theme of the following text:\\n{text}\"\n",
    ")\n",
    "\n",
    "tone_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Describe the overall tone of the following text:\\n{text}\"\n",
    ")\n",
    "\n",
    "takeaway_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"theme\", \"tone\"],\n",
    "    template=\"Given the following text with the main theme '{theme}' and tone '{tone}', what are the key takeaways?\\n{text}\"\n",
    ")\n",
    "\n",
    "def analyze_text(text):\n",
    "    \"\"\"Perform a multi-step analysis of a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the theme, tone, and key takeaways of the text.\n",
    "    \"\"\"\n",
    "    theme = (theme_prompt | llm).invoke({\"text\": text}).content\n",
    "    tone = (tone_prompt | llm).invoke({\"text\": text}).content\n",
    "    takeaways = (takeaway_prompt | llm).invoke({\"text\": text, \"theme\": theme, \"tone\": tone}).content\n",
    "    return {\"theme\": theme, \"tone\": tone, \"takeaways\": takeaways}\n",
    "\n",
    "# Test the sequential prompting\n",
    "sample_text = \"The rapid advancement of artificial intelligence has sparked both excitement and concern among experts. While AI promises to revolutionize industries and improve our daily lives, it also raises ethical questions about privacy, job displacement, and the potential for misuse. As we stand on the brink of this technological revolution, it's crucial that we approach AI development with caution and foresight, ensuring that its benefits are maximized while its risks are minimized.\"\n",
    "\n",
    "analysis = analyze_text(sample_text)\n",
    "for key, value in analysis.items():\n",
    "    print(f\"{key.capitalize()}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a4674",
   "metadata": {},
   "source": [
    "### Dynamic Prompt Generation\n",
    "In this section, we'll create a dynamic question-answering system that generates follow-up questions based on previous answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04748fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: What are the potential applications of quantum computing?\n",
      "A1: Drug discovery, materials science, financial modeling, cryptography, and optimization problems.\n",
      "\n",
      "Q2: Given those answers, a relevant follow-up question could be:\n",
      "\n",
      "**\"For each of those applications (drug discovery, materials science, financial modeling, cryptography, and optimization problems), what specific limitations currently prevent classical computers from fully addressing those challenges, and how might quantum computing overcome them?\"**\n",
      "\n",
      "This question digs deeper and asks *why* quantum computing is needed in those areas, rather than just listing them. It also encourages a discussion of the specific computational bottlenecks that quantum computers could potentially solve.\n",
      "A2: Okay, I understand. My answer is:\n",
      "\n",
      "Drug discovery, materials science, financial modeling, cryptography, and optimization problems.\n",
      "\n",
      "Q3: Okay, given my initial answer and the established context, a relevant follow-up question could be:\n",
      "\n",
      "**\"Beyond the potential advantages of quantum computing you foresee in these areas, what are the *realistic* timelines for seeing significant breakthroughs in each field that are directly attributable to quantum computation, considering current quantum hardware limitations and the ongoing need for algorithm development?\"**\n",
      "\n",
      "This question builds on the previous one by:\n",
      "\n",
      "*   **Acknowledging limitations:** It directly addresses the fact that quantum computers are not a magic bullet and have current limitations.\n",
      "*   **Focusing on realistic timelines:** It moves beyond theoretical possibilities and asks for a practical assessment of when we might see tangible results.\n",
      "*   **Emphasizing hardware and algorithm development:** It highlights the two key areas that need to advance for quantum computing to become truly useful.\n",
      "*   **Demanding specificity:** It asks for a separate estimate for each of the initially listed applications, recognizing that progress might vary significantly between them.\n",
      "A3: Okay, the follow-up question is asking for **realistic timelines for quantum computing breakthroughs in specific fields, considering current limitations and the need for hardware and algorithm development.**\n",
      "\n",
      "Q4: Okay, given my initial answer, the established context, and the previous follow-up question (and its corresponding analysis), a relevant follow-up question could be:\n",
      "\n",
      "**\"Considering the inherent probabilistic nature of quantum algorithms and the potential for error accumulation due to decoherence, what specific error mitigation or correction techniques are you prioritizing in the development of algorithms for each of these target fields, and how might the success (or failure) of these techniques influence your previously stated timelines for significant breakthroughs?\"**\n",
      "\n",
      "This question builds upon the previous ones by:\n",
      "\n",
      "*   **Addressing a core challenge:** It dives into the crucial issue of quantum error correction/mitigation, a significant hurdle to achieving practical quantum computation.\n",
      "*   **Linking error correction to timelines:** It explicitly connects the success of error mitigation strategies to the previously discussed realistic timelines.\n",
      "*   **Emphasizing specificity to target fields:** It maintains the focus on individual applications, recognizing that different fields might require different approaches to error management.\n",
      "*   **Focusing on active strategies:** It asks about *prioritized* techniques, prompting the responder to reveal their specific research focus in error mitigation.\n",
      "*   **Highlighting potential impact:** It implicitly asks for a contingency plan – how the timelines might change if specific error correction approaches prove ineffective.\n",
      "A4: Okay, I understand. What is the question you want me to answer?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question concisely:\\n{question}\"\n",
    ")\n",
    "\n",
    "follow_up_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Based on the question '{question}' and the answer '{answer}', generate a relevant follow-up question.\"\n",
    ")\n",
    "\n",
    "def dynamic_qa(initial_question, num_follow_ups=3):\n",
    "    \"\"\"Conduct a dynamic Q&A session with follow-up questions.\n",
    "\n",
    "    Args:\n",
    "        initial_question (str): The initial question to start the Q&A session.\n",
    "        num_follow_ups (int): The number of follow-up questions to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing questions and answers.\n",
    "    \"\"\"\n",
    "    qa_chain = []\n",
    "    current_question = initial_question\n",
    "\n",
    "    for _ in range(num_follow_ups + 1):  # +1 for the initial question\n",
    "        answer = (answer_prompt | llm).invoke({\"question\": current_question}).content\n",
    "        qa_chain.append({\"question\": current_question, \"answer\": answer})\n",
    "        \n",
    "        if _ < num_follow_ups:  # Generate follow-up for all but the last iteration\n",
    "            current_question = (follow_up_prompt | llm).invoke({\"question\": current_question, \"answer\": answer}).content\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "# Test the dynamic Q&A system\n",
    "initial_question = \"What are the potential applications of quantum computing?\"\n",
    "qa_session = dynamic_qa(initial_question)\n",
    "\n",
    "for i, qa in enumerate(qa_session):\n",
    "    print(f\"Q{i+1}: {qa['question']}\")\n",
    "    print(f\"A{i+1}: {qa['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cf9d8",
   "metadata": {},
   "source": [
    "### Error Handling and Validation\n",
    "In this final section, we'll implement error handling and validation in our prompt chains to make them more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f28bafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result for topic 'World War II': 1945\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Define prompt templates\n",
    "generate_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a 4-digit number related to the topic: {topic}. Respond with ONLY the number, no additional text.\"\n",
    ")\n",
    "\n",
    "validate_prompt = PromptTemplate(\n",
    "    input_variables=[\"number\", \"topic\"],\n",
    "    template=\"Is the number {number} truly related to the topic '{topic}'? Answer with 'Yes' or 'No' and explain why.\"\n",
    ")\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Extract a 4-digit number from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to extract the number from.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The extracted 4-digit number, or None if no valid number is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\b\\d{4}\\b', text)\n",
    "    return match.group() if match else None\n",
    "\n",
    "def robust_number_generation(topic, max_attempts=3):\n",
    "    \"\"\"Generate a topic-related number with validation and error handling.\n",
    "\n",
    "    Args:\n",
    "        topic (str): The topic to generate a number for.\n",
    "        max_attempts (int): Maximum number of generation attempts.\n",
    "\n",
    "    Returns:\n",
    "        str: A validated 4-digit number or an error message.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = (generate_prompt | llm).invoke({\"topic\": topic}).content\n",
    "            number = extract_number(response)\n",
    "            \n",
    "            if not number:\n",
    "                raise ValueError(f\"Failed to extract a 4-digit number from the response: {response}\")\n",
    "            \n",
    "            # Validate the relevance\n",
    "            validation = (validate_prompt | llm).invoke({\"number\": number, \"topic\": topic}).content\n",
    "            if validation.lower().startswith(\"yes\"):\n",
    "                return number\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}: Number {number} was not validated. Reason: {validation}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "    \n",
    "    return \"Failed to generate a valid number after multiple attempts.\"\n",
    "\n",
    "# Test the robust number generation\n",
    "topic = \"World War II\"\n",
    "result = robust_number_generation(topic)\n",
    "print(f\"Final result for topic '{topic}': {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
