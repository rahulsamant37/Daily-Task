{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a544ef2a",
   "metadata": {},
   "source": [
    "# Evaluating Prompt Effectiveness\n",
    "### Overview\n",
    "This tutorial focuses on methods and techniques for evaluating the effectiveness of prompts in AI language models. We'll explore various metrics for measuring prompt performance and discuss both manual and automated evaluation techniques.\n",
    "\n",
    "### Motivation\n",
    "As prompt engineering becomes increasingly crucial in AI applications, it's essential to have robust methods for assessing prompt effectiveness. This enables developers and researchers to optimize their prompts, leading to better AI model performance and more reliable outputs.\n",
    "\n",
    "### Key Components\n",
    "1. Metrics for measuring prompt performance\n",
    "2. Manual evaluation techniques\n",
    "3. Automated evaluation techniques\n",
    "4. Practical examples using Gemini and LangChain\n",
    "### Method Details\n",
    "We'll start by setting up our environment and introducing key metrics for evaluating prompts. We'll then explore manual evaluation techniques, including human assessment and comparative analysis. Next, we'll delve into automated evaluation methods, utilizing techniques like perplexity scoring and automated semantic similarity comparisons. Throughout the tutorial, we'll provide practical examples using Gemini models and LangChain library to demonstrate these concepts in action.\n",
    "\n",
    "### Conclusion\n",
    "By the end of this tutorial, you'll have a comprehensive understanding of how to evaluate prompt effectiveness using both manual and automated techniques. You'll be equipped with practical tools and methods to optimize your prompts, leading to more efficient and accurate AI model interactions.\n",
    "\n",
    "### Setup\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f118b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load enviroment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Google API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ['HF_TOKEN']=os.getenv('HF_TOKEN')\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def semantic_similarity(text1, text2):\n",
    "    \"\"\"Calculate semantic similarity between two texts using cosine similarity.\"\"\"\n",
    "    # Generate embeddings for both texts\n",
    "    embeddings1 = embedding.embed_query(text1)\n",
    "    embeddings2 = embedding.embed_query(text2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    return cosine_similarity([embeddings1], [embeddings2])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c1c1f",
   "metadata": {},
   "source": [
    "### Metrics for Measuring Prompt Performance\n",
    "Let's define some key metrics for evaluating prompt effectiveness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce707a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_score(response, expected_content):\n",
    "    \"\"\"Calculate relevance score based on semantic similarity to expected content.\"\"\"\n",
    "    return semantic_similarity(response, expected_content)\n",
    "\n",
    "def consistency_score(responses):\n",
    "    \"\"\"Calculate consistency score based on similarity between multiple responses.\"\"\"\n",
    "    if len(responses) < 2:\n",
    "        return 1.0  # Perfect consistency if there's only one response\n",
    "    similarities = []\n",
    "    for i in range(len(responses)):\n",
    "        for j in range(i+1, len(responses)):\n",
    "            similarities.append(semantic_similarity(responses[i], responses[j]))\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def specificity_score(response):\n",
    "    \"\"\"Calculate specificity score based on response length and unique word count.\"\"\"\n",
    "    words = response.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8bbbe",
   "metadata": {},
   "source": [
    "### Manual Evaluation Techniques\n",
    "Manual evaluation involves human assessment of prompt-response pairs. Let's create a function to simulate this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1794e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the concept of machine learning in simple terms.\n",
      "Response: Imagine you're teaching a dog a trick. You show it what you want it to do, and when it gets it right, you give it a treat. The dog learns by seeing examples and getting feedback.\n",
      "\n",
      "Machine learning is similar, but instead of a dog, it's a computer.  We give the computer lots of data (like examples) and tell it what we want it to learn.  The computer then analyzes the data and tries to find patterns and rules.  \n",
      "\n",
      "Instead of treats, the computer gets better at making predictions or decisions. The more data it sees, the better it gets.\n",
      "\n",
      "**Here's a breakdown:**\n",
      "\n",
      "* **Data:**  The \"examples\" you give the computer. This could be anything from pictures of cats and dogs to sales figures from the past year.\n",
      "* **Learning:** The process of the computer finding patterns and relationships in the data.\n",
      "* **Model:** The \"trick\" the computer learns. It's a set of rules or a formula that helps it make predictions or decisions.\n",
      "* **Prediction/Decision:** What the computer does with its learned model.  For example, it might predict the weather tomorrow, identify a cat in a picture, or recommend a product you might like.\n",
      "\n",
      "**In short, machine learning is about teaching computers to learn from data without being explicitly programmed to do everything.**  Instead of telling the computer exactly what to do in every situation, we give it the data and let it figure out the best way to solve the problem.\n",
      "\n",
      "**Examples of Machine Learning in action:**\n",
      "\n",
      "* **Spam filters:**  They learn to identify spam emails based on the words and phrases they contain.\n",
      "* **Recommendation systems (like Netflix or Amazon):** They learn your preferences based on what you've watched or bought in the past and recommend similar items.\n",
      "* **Self-driving cars:** They learn to navigate roads and avoid obstacles by analyzing data from sensors and cameras.\n",
      "* **Medical diagnosis:**  Machine learning can help doctors diagnose diseases by analyzing medical images and patient data.\n",
      "\n",
      "Evaluation Criteria:\n",
      "Clarity: 7.0/10\n",
      "Accuracy: 8.0/10\n",
      "Simplicity: 10.0/10\n",
      "\n",
      "Additional Comments:\n",
      "Comments: 10\n"
     ]
    }
   ],
   "source": [
    "def manual_evaluation(prompt, response, criteria):\n",
    "    \"\"\"Simulate manual evaluation of a prompt-response pair.\"\"\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"\\nEvaluation Criteria:\")\n",
    "    for criterion in criteria:\n",
    "        score = float(input(f\"Score for {criterion} (0-10): \"))\n",
    "        print(f\"{criterion}: {score}/10\")\n",
    "    print(\"\\nAdditional Comments:\")\n",
    "    comments = input(\"Enter any additional comments: \")\n",
    "    print(f\"Comments: {comments}\")\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain the concept of machine learning in simple terms.\"\n",
    "response = llm.invoke(prompt).content\n",
    "criteria = [\"Clarity\", \"Accuracy\", \"Simplicity\"]\n",
    "manual_evaluation(prompt, response, criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a35d8",
   "metadata": {},
   "source": [
    "### Automated Evaluation Techniques\n",
    "Now, let's implement some automated evaluation techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4cb8932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What are the three main types of machine learning?\n",
      "Response: The three main types of machine learning are:\n",
      "\n",
      "1.  **Supervised Learning:** In supervised learning, the algorithm is trained on a labeled dataset, meaning the data includes both the input features and the desired output (the \"label\"). The algorithm learns a mapping function that can predict the output for new, unseen input data.  Think of it like learning with a teacher who provides the correct answers.\n",
      "\n",
      "    *   **Examples:** Image classification (identifying objects in images), spam detection (classifying emails as spam or not spam), regression (predicting house prices based on features like size and location).\n",
      "\n",
      "2.  **Unsupervised Learning:**  In unsupervised learning, the algorithm is trained on an unlabeled dataset, meaning the data only includes the input features without any corresponding output labels. The algorithm aims to discover hidden patterns, structures, or relationships within the data without any prior guidance. It's like exploring a new territory without a map.\n",
      "\n",
      "    *   **Examples:** Clustering (grouping customers into segments based on purchasing behavior), anomaly detection (identifying unusual transactions in a financial dataset), dimensionality reduction (reducing the number of features in a dataset while preserving important information).\n",
      "\n",
      "3.  **Reinforcement Learning:** In reinforcement learning, an agent learns to make decisions in an environment to maximize a reward. The agent interacts with the environment, receives feedback in the form of rewards or penalties, and adjusts its actions to learn an optimal policy (a strategy for making decisions). It's like training a dog with treats.\n",
      "\n",
      "    *   **Examples:** Training a robot to navigate a room, developing game-playing AI (like AlphaGo), optimizing advertising campaigns.\n",
      "\n",
      "Relevance Score: 0.75\n",
      "Specificity Score: 0.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevance': 0.7502844021521002, 'specificity': 0.6428571428571429}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def automated_evaluation(prompt, response, expected_content):\n",
    "    \"\"\"Perform automated evaluation of a prompt-response pair.\"\"\"\n",
    "    relevance = relevance_score(response, expected_content)\n",
    "    specificity = specificity_score(response)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"\\nRelevance Score: {relevance:.2f}\")\n",
    "    print(f\"Specificity Score: {specificity:.2f}\")\n",
    "    \n",
    "    return {\"relevance\": relevance, \"specificity\": specificity}\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What are the three main types of machine learning?\"\n",
    "expected_content = \"The three main types of machine learning are supervised learning, unsupervised learning, and reinforcement learning.\"\n",
    "response = llm.invoke(prompt).content\n",
    "automated_evaluation(prompt, response, expected_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897930f",
   "metadata": {},
   "source": [
    "### Comparative Analysis\n",
    "Let's compare the effectiveness of different prompts for the same task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90397e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: List the types of machine learning.\n",
      "Response: Machine learning can be broadly categorized into the following types:\n",
      "\n",
      "**1. Supervised Learning:**\n",
      "\n",
      "*   **Definition:** Learns from labeled data, where the input features and the corresponding output are provided. The algorithm learns a mapping function to predict the output for new, unseen inputs.\n",
      "*   **Types of Problems:**\n",
      "    *   **Classification:** Predicts a categorical output (e.g., spam or not spam, cat or dog).\n",
      "    *   **Regression:** Predicts a continuous output (e.g., house price, temperature).\n",
      "*   **Common Algorithms:**\n",
      "    *   Linear Regression\n",
      "    *   Logistic Regression\n",
      "    *   Support Vector Machines (SVM)\n",
      "    *   Decision Trees\n",
      "    *   Random Forest\n",
      "    *   K-Nearest Neighbors (KNN)\n",
      "    *   Naive Bayes\n",
      "    *   Neural Networks (e.g., Multi-Layer Perceptron)\n",
      "\n",
      "**2. Unsupervised Learning:**\n",
      "\n",
      "*   **Definition:** Learns from unlabeled data, where only the input features are provided. The algorithm aims to discover hidden patterns, structures, or relationships within the data.\n",
      "*   **Types of Problems:**\n",
      "    *   **Clustering:** Groups similar data points together (e.g., customer segmentation).\n",
      "    *   **Dimensionality Reduction:** Reduces the number of features while preserving important information (e.g., Principal Component Analysis).\n",
      "    *   **Association Rule Mining:** Discovers relationships between variables (e.g., market basket analysis).\n",
      "    *   **Anomaly Detection:** Identifies unusual or outlier data points.\n",
      "*   **Common Algorithms:**\n",
      "    *   K-Means Clustering\n",
      "    *   Hierarchical Clustering\n",
      "    *   Principal Component Analysis (PCA)\n",
      "    *   Independent Component Analysis (ICA)\n",
      "    *   Association Rule Learning (e.g., Apriori, Eclat)\n",
      "    *   One-Class SVM\n",
      "    *   Isolation Forest\n",
      "\n",
      "**3. Semi-Supervised Learning:**\n",
      "\n",
      "*   **Definition:** Learns from a combination of labeled and unlabeled data.  This is useful when labeling data is expensive or time-consuming.\n",
      "*   **Approach:** The algorithm leverages the labeled data to learn a model and then uses that model to predict labels for the unlabeled data, which are then used to further refine the model.\n",
      "*   **Common Algorithms:**\n",
      "    *   Self-Training\n",
      "    *   Co-Training\n",
      "    *   Label Propagation\n",
      "    *   Generative Models (e.g., Variational Autoencoders) with semi-supervised objectives\n",
      "\n",
      "**4. Reinforcement Learning:**\n",
      "\n",
      "*   **Definition:** Learns through trial and error by interacting with an environment. The algorithm (agent) receives feedback in the form of rewards or penalties and aims to learn a policy that maximizes the cumulative reward.\n",
      "*   **Key Concepts:**\n",
      "    *   **Agent:** The learner.\n",
      "    *   **Environment:** The world the agent interacts with.\n",
      "    *   **State:** The current situation of the agent in the environment.\n",
      "    *   **Action:** A choice the agent makes in a given state.\n",
      "    *   **Reward:** Feedback received after taking an action.\n",
      "    *   **Policy:** A mapping from states to actions.\n",
      "*   **Common Algorithms:**\n",
      "    *   Q-Learning\n",
      "    *   SARSA (State-Action-Reward-State-Action)\n",
      "    *   Deep Q-Network (DQN)\n",
      "    *   Policy Gradient Methods (e.g., REINFORCE, Actor-Critic)\n",
      "\n",
      "**5. Self-Supervised Learning:**\n",
      "\n",
      "*   **Definition:**  A type of unsupervised learning where the algorithm generates its own labels from the unlabeled data.  It's often used for representation learning.\n",
      "*   **Approach:**  The algorithm creates a pretext task (e.g., predicting a masked word in a sentence, predicting the rotation of an image) and learns to solve that task.  The learned representations are then used for downstream tasks.\n",
      "*   **Examples:**\n",
      "    *   **Natural Language Processing (NLP):** Masked language modeling (BERT), next sentence prediction.\n",
      "    *   **Computer Vision:** Image colorization, image rotation prediction, contrastive learning.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Batch Learning:** All training data is available at once.\n",
      "*   **Online Learning:** Data is presented sequentially, and the model is updated incrementally.  Useful when data arrives in a stream.\n",
      "*   **Transfer Learning:**  Leveraging knowledge gained from solving one problem to solve a different but related problem.  This can significantly reduce training time and improve performance.\n",
      "*   **Active Learning:**  The algorithm selectively queries for labels of the most informative data points to improve learning efficiency.\n",
      "\n",
      "This list covers the main types of machine learning. It's important to understand the strengths and weaknesses of each type to choose the right approach for a given problem.  The field is constantly evolving, and new techniques and variations are continually being developed.\n",
      "\n",
      "Relevance Score: 0.69\n",
      "Specificity Score: 0.55\n",
      "Prompt: What are the main categories of machine learning algorithms?\n",
      "Response: The main categories of machine learning algorithms are:\n",
      "\n",
      "**1. Supervised Learning:**\n",
      "\n",
      "*   **Definition:** Algorithms learn from labeled data, where the input features and the corresponding output (target variable) are provided. The goal is to learn a mapping function that can predict the output for new, unseen input data.\n",
      "*   **Types:**\n",
      "    *   **Classification:**  The output is a categorical variable (e.g., spam/not spam, cat/dog/bird).\n",
      "        *   Examples: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forests, Naive Bayes, K-Nearest Neighbors (KNN), Neural Networks (e.g., Multilayer Perceptron - MLP).\n",
      "    *   **Regression:** The output is a continuous variable (e.g., price, temperature, salary).\n",
      "        *   Examples: Linear Regression, Polynomial Regression, Support Vector Regression (SVR), Decision Trees, Random Forests, Neural Networks, Lasso Regression, Ridge Regression.\n",
      "\n",
      "**2. Unsupervised Learning:**\n",
      "\n",
      "*   **Definition:** Algorithms learn from unlabeled data, where only the input features are provided. The goal is to discover hidden patterns, structures, or relationships within the data.\n",
      "*   **Types:**\n",
      "    *   **Clustering:** Grouping similar data points together into clusters.\n",
      "        *   Examples: K-Means Clustering, Hierarchical Clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), Gaussian Mixture Models (GMM).\n",
      "    *   **Dimensionality Reduction:** Reducing the number of features while preserving important information.\n",
      "        *   Examples: Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), Linear Discriminant Analysis (LDA), Autoencoders.\n",
      "    *   **Association Rule Mining:** Discovering relationships between items in a dataset (often used in market basket analysis).\n",
      "        *   Examples: Apriori Algorithm, Eclat Algorithm, FP-Growth Algorithm.\n",
      "\n",
      "**3. Reinforcement Learning:**\n",
      "\n",
      "*   **Definition:** An agent learns to make decisions in an environment to maximize a reward. The agent interacts with the environment, receives feedback (rewards or penalties), and adjusts its actions accordingly.\n",
      "*   **Key Concepts:**\n",
      "    *   **Agent:** The learner.\n",
      "    *   **Environment:** The context in which the agent operates.\n",
      "    *   **Action:** A choice made by the agent.\n",
      "    *   **State:** The current situation of the agent in the environment.\n",
      "    *   **Reward:** Feedback from the environment, indicating the desirability of an action.\n",
      "    *   **Policy:** A strategy that maps states to actions.\n",
      "*   **Types:**\n",
      "    *   **Q-Learning:** Learns a Q-function that estimates the expected cumulative reward for taking a specific action in a specific state.\n",
      "    *   **SARSA (State-Action-Reward-State-Action):**  An on-policy algorithm that updates the Q-function based on the action the agent actually takes.\n",
      "    *   **Deep Reinforcement Learning:** Uses deep neural networks to approximate the Q-function or policy.  Examples include Deep Q-Networks (DQN) and Policy Gradient methods (e.g., REINFORCE, A2C, A3C, PPO).\n",
      "\n",
      "**4. Semi-Supervised Learning:**\n",
      "\n",
      "*   **Definition:** A combination of supervised and unsupervised learning. The algorithm learns from a dataset that contains both labeled and unlabeled data.  This is useful when labeling data is expensive or time-consuming.\n",
      "*   **Techniques:**\n",
      "    *   Self-training\n",
      "    *   Co-training\n",
      "    *   Label propagation\n",
      "    *   Generative models (e.g., using a GAN to generate labeled data)\n",
      "\n",
      "**5. Self-Supervised Learning:**\n",
      "\n",
      "*   **Definition:**  A type of unsupervised learning where the algorithm learns from data that is inherently structured.  The structure is used to create pseudo-labels, which are then used in a supervised learning manner.  The algorithm learns to predict parts of the input from other parts of the input.\n",
      "*   **Example:** Predicting the next word in a sentence (language models).  This doesn't require external labels because the surrounding words provide the \"ground truth.\"\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Model Selection:** The choice of algorithm depends on the type of problem, the nature of the data, and the desired outcome.\n",
      "*   **Data Preprocessing:** Data cleaning, transformation, and feature engineering are crucial steps in preparing data for machine learning algorithms.\n",
      "*   **Evaluation:** Evaluating the performance of a model is essential to ensure its effectiveness and generalization ability. Metrics vary depending on the type of problem (e.g., accuracy, precision, recall, F1-score for classification; mean squared error, R-squared for regression).\n",
      "*   **Overfitting and Underfitting:**  These are common problems that need to be addressed during model training. Overfitting occurs when the model learns the training data too well and performs poorly on unseen data. Underfitting occurs when the model is too simple to capture the underlying patterns in the data.  Techniques like regularization, cross-validation, and using more data can help to mitigate these issues.\n",
      "\n",
      "This categorization is not always strict, and some algorithms can be used in multiple categories. For example, neural networks can be used for both supervised and unsupervised learning.  Furthermore, new algorithms and techniques are constantly being developed.\n",
      "\n",
      "Relevance Score: 0.71\n",
      "Specificity Score: 0.56\n",
      "Prompt: Explain the different approaches to machine learning.\n",
      "Response: Machine learning is a vast field, and there are many different approaches to solving problems using machine learning techniques. Here's a breakdown of the most common and important approaches, categorized by their primary learning style:\n",
      "\n",
      "**1. Supervised Learning:**\n",
      "\n",
      "*   **Concept:** Learns from labeled data, meaning the data has input features and corresponding output labels (target variables). The goal is to learn a function that maps inputs to outputs. Think of it like learning from a teacher who provides the correct answers.\n",
      "*   **Key Characteristics:**\n",
      "    *   **Labeled Data:** Requires a dataset where each example is paired with the correct answer.\n",
      "    *   **Prediction:** Used for predicting outputs for new, unseen inputs.\n",
      "    *   **Examples:**  Predicting house prices based on features like size and location, classifying emails as spam or not spam, identifying objects in an image.\n",
      "\n",
      "*   **Common Algorithms:**\n",
      "    *   **Regression:** Predicts continuous values.\n",
      "        *   *Linear Regression:*  Models the relationship between variables using a straight line (or hyperplane in higher dimensions).\n",
      "        *   *Polynomial Regression:* Models the relationship using a polynomial curve.\n",
      "        *   *Support Vector Regression (SVR):*  Uses support vectors to find the best fitting hyperplane within a margin of error.\n",
      "        *   *Decision Tree Regression:* Builds a tree-like structure to predict values based on decisions.\n",
      "        *   *Random Forest Regression:*  An ensemble of decision trees to improve accuracy and reduce overfitting.\n",
      "    *   **Classification:** Predicts categorical values (classes or categories).\n",
      "        *   *Logistic Regression:*  Uses a sigmoid function to predict probabilities for binary classification.\n",
      "        *   *Support Vector Machines (SVM):*  Finds the optimal hyperplane to separate data points into different classes.\n",
      "        *   *Decision Tree Classification:*  Builds a tree-like structure to classify data based on decisions.\n",
      "        *   *Random Forest Classification:*  An ensemble of decision trees to improve accuracy and reduce overfitting.\n",
      "        *   *Naive Bayes:*  Applies Bayes' theorem with strong (naive) independence assumptions between features.\n",
      "        *   *K-Nearest Neighbors (KNN):*  Classifies a data point based on the majority class of its k nearest neighbors.\n",
      "        *   *Neural Networks (Multi-Layer Perceptron - MLP):*  Complex models inspired by the structure of the human brain, capable of learning complex patterns.\n",
      "\n",
      "**2. Unsupervised Learning:**\n",
      "\n",
      "*   **Concept:** Learns from unlabeled data, meaning the data only has input features without corresponding output labels. The goal is to discover hidden patterns, structures, and relationships within the data.  Think of it like exploring a new land without a guide.\n",
      "*   **Key Characteristics:**\n",
      "    *   **Unlabeled Data:** Deals with datasets where the correct answers are not provided.\n",
      "    *   **Discovery:** Used for finding patterns, grouping similar data points, and reducing dimensionality.\n",
      "    *   **Examples:** Customer segmentation, anomaly detection, topic modeling in text documents, image compression.\n",
      "\n",
      "*   **Common Algorithms:**\n",
      "    *   **Clustering:** Groups similar data points together.\n",
      "        *   *K-Means Clustering:*  Partitions data into k clusters, where each data point belongs to the cluster with the nearest mean (centroid).\n",
      "        *   *Hierarchical Clustering:*  Builds a hierarchy of clusters, either bottom-up (agglomerative) or top-down (divisive).\n",
      "        *   *DBSCAN (Density-Based Spatial Clustering of Applications with Noise):* Groups together data points that are closely packed together, marking as outliers points that lie alone in low-density regions.\n",
      "        *   *Gaussian Mixture Models (GMM):* Assumes that the data is generated from a mixture of Gaussian distributions.\n",
      "    *   **Dimensionality Reduction:** Reduces the number of features while preserving important information.\n",
      "        *   *Principal Component Analysis (PCA):*  Identifies the principal components (directions of maximum variance) in the data.\n",
      "        *   *t-distributed Stochastic Neighbor Embedding (t-SNE):*  Reduces dimensionality while preserving the local structure of the data, often used for visualizing high-dimensional data.\n",
      "        *   *Autoencoders:* Neural networks trained to reconstruct the input, forcing the network to learn a compressed representation in the middle layer.\n",
      "    *   **Association Rule Mining:**  Discovers relationships between items in a dataset.\n",
      "        *   *Apriori Algorithm:*  Finds frequent itemsets in a transaction database and generates association rules based on these itemsets.\n",
      "\n",
      "**3. Semi-Supervised Learning:**\n",
      "\n",
      "*   **Concept:** Uses a combination of labeled and unlabeled data for training. This is particularly useful when labeling data is expensive or time-consuming.\n",
      "*   **Key Characteristics:**\n",
      "    *   **Mixed Data:** Utilizes both labeled and unlabeled data.\n",
      "    *   **Leveraging Unlabeled Data:** Improves performance by leveraging the information in the unlabeled data to complement the labeled data.\n",
      "    *   **Examples:** Document classification where only a small portion of documents are labeled, image classification where only a subset of images are labeled.\n",
      "\n",
      "*   **Common Algorithms:**\n",
      "    *   **Self-Training:**  Trains a model on the labeled data, then uses the model to predict labels for the unlabeled data.  The most confident predictions are then added to the labeled dataset, and the model is retrained.\n",
      "    *   **Generative Models:**  Models that learn the underlying distribution of the data and can then be used to generate new data points or to infer labels for unlabeled data.\n",
      "    *   **Graph-Based Methods:**  Represent data as a graph, where nodes are data points and edges represent relationships between them.  Labels can then be propagated through the graph to unlabeled nodes.\n",
      "\n",
      "**4. Reinforcement Learning:**\n",
      "\n",
      "*   **Concept:** An agent learns to make decisions in an environment to maximize a reward. The agent receives feedback in the form of rewards or penalties for its actions. Think of it like training a dog with treats and scolding.\n",
      "*   **Key Characteristics:**\n",
      "    *   **Agent-Environment Interaction:**  An agent interacts with an environment.\n",
      "    *   **Reward Signal:**  The agent receives a reward signal for its actions.\n",
      "    *   **Learning Policy:**  The goal is to learn a policy that maps states to actions to maximize the cumulative reward.\n",
      "    *   **Examples:** Training a robot to navigate a room, playing games like Go or chess, optimizing advertising campaigns.\n",
      "\n",
      "*   **Common Algorithms:**\n",
      "    *   **Q-Learning:**  Learns a Q-function that estimates the expected cumulative reward for taking a specific action in a specific state.\n",
      "    *   **SARSA (State-Action-Reward-State-Action):** An on-policy algorithm that updates the Q-function based on the actions taken by the current policy.\n",
      "    *   **Deep Q-Networks (DQN):**  Uses deep neural networks to approximate the Q-function, allowing it to handle complex environments with high-dimensional state spaces.\n",
      "    *   **Policy Gradient Methods:**  Directly optimize the policy without learning a Q-function.\n",
      "    *   **Actor-Critic Methods:**  Combine policy gradient and Q-learning methods, using an actor to learn the policy and a critic to evaluate the policy.\n",
      "\n",
      "**5. Deep Learning:**\n",
      "\n",
      "*   **Concept:** A subfield of machine learning that uses artificial neural networks with multiple layers (deep neural networks) to analyze data. These networks are capable of learning complex patterns and representations from large amounts of data.\n",
      "*   **Key Characteristics:**\n",
      "    *   **Multi-Layer Neural Networks:**  Uses neural networks with many layers (typically more than three).\n",
      "    *   **Feature Learning:**  Automatically learns features from raw data, reducing the need for manual feature engineering.\n",
      "    *   **Data-Intensive:**  Requires large amounts of data for training.\n",
      "    *   **Computationally Intensive:**  Requires significant computational resources for training.\n",
      "    *   **Examples:** Image recognition, natural language processing, speech recognition, machine translation.\n",
      "\n",
      "*   **Common Architectures:**\n",
      "    *   **Convolutional Neural Networks (CNNs):**  Designed for processing images and videos.\n",
      "    *   **Recurrent Neural Networks (RNNs):**  Designed for processing sequential data, such as text and time series.\n",
      "    *   **Long Short-Term Memory (LSTM) networks:** A type of RNN that can handle long-range dependencies in sequential data.\n",
      "    *   **Transformers:** A powerful architecture based on attention mechanisms, widely used in natural language processing.\n",
      "    *   **Generative Adversarial Networks (GANs):**  Used for generating new data that resembles the training data.\n",
      "\n",
      "**Other Important Considerations:**\n",
      "\n",
      "*   **Ensemble Learning:** Combines multiple machine learning models to improve performance. Common techniques include bagging, boosting, and stacking.  Random Forests are a common example.\n",
      "*   **Transfer Learning:**  Reuses knowledge gained from solving one problem to solve a different but related problem. This can be particularly useful when data is scarce for the new problem.\n",
      "*   **Active Learning:**  A learning approach where the algorithm actively queries the user or annotator to label the most informative data points. This can reduce the amount of labeled data required for training.\n",
      "\n",
      "**Choosing the Right Approach:**\n",
      "\n",
      "The best machine learning approach depends on several factors, including:\n",
      "\n",
      "*   **The type of data available:**  Is the data labeled or unlabeled? Is it sequential or static?\n",
      "*   **The problem you are trying to solve:**  Are you trying to predict a value, classify data, find patterns, or make decisions?\n",
      "*   **The amount of data available:**  Deep learning models typically require large amounts of data.\n",
      "*   **The computational resources available:**  Deep learning models can be computationally expensive to train.\n",
      "*   **The desired accuracy and interpretability:**  Some models are more accurate than others, but they may also be more difficult to interpret.\n",
      "\n",
      "Understanding the strengths and weaknesses of each approach is crucial for selecting the right tool for the job.  It's also common to experiment with different approaches to find the one that performs best for a particular problem.\n",
      "\n",
      "Relevance Score: 0.66\n",
      "Specificity Score: 0.46\n",
      "Prompt Comparison Results:\n",
      "\n",
      "1. Prompt: What are the main categories of machine learning algorithms?\n",
      "   Relevance: 0.71\n",
      "   Specificity: 0.56\n",
      "\n",
      "2. Prompt: List the types of machine learning.\n",
      "   Relevance: 0.69\n",
      "   Specificity: 0.55\n",
      "\n",
      "3. Prompt: Explain the different approaches to machine learning.\n",
      "   Relevance: 0.66\n",
      "   Specificity: 0.46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'What are the main categories of machine learning algorithms?',\n",
       "  'relevance': 0.7058402065193237,\n",
       "  'specificity': 0.5550786838340487},\n",
       " {'prompt': 'List the types of machine learning.',\n",
       "  'relevance': 0.6883750515394957,\n",
       "  'specificity': 0.5508196721311476},\n",
       " {'prompt': 'Explain the different approaches to machine learning.',\n",
       "  'relevance': 0.6571559594837572,\n",
       "  'specificity': 0.4625801853171775}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_prompts(prompts, expected_content):\n",
    "    \"\"\"Compare the effectiveness of multiple prompts for the same task.\"\"\"\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        response = llm.invoke(prompt).content\n",
    "        evaluation = automated_evaluation(prompt, response, expected_content)\n",
    "        results.append({\"prompt\": prompt, **evaluation})\n",
    "    \n",
    "    # Sort results by relevance score\n",
    "    sorted_results = sorted(results, key=lambda x: x['relevance'], reverse=True)\n",
    "    \n",
    "    print(\"Prompt Comparison Results:\")\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"\\n{i}. Prompt: {result['prompt']}\")\n",
    "        print(f\"   Relevance: {result['relevance']:.2f}\")\n",
    "        print(f\"   Specificity: {result['specificity']:.2f}\")\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "# Example usage\n",
    "prompts = [\n",
    "    \"List the types of machine learning.\",\n",
    "    \"What are the main categories of machine learning algorithms?\",\n",
    "    \"Explain the different approaches to machine learning.\"\n",
    "]\n",
    "expected_content = \"The main types of machine learning are supervised learning, unsupervised learning, and reinforcement learning.\"\n",
    "compare_prompts(prompts, expected_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37f935",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "Now, let's create a comprehensive prompt evaluation function that combines both manual and automated techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb75377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Evaluation:\n",
      "Prompt: Explain the concept of overfitting in machine learning.\n",
      "Response: ## Overfitting in Machine Learning: A Detailed Explanation\n",
      "\n",
      "Overfitting is a common problem in machine learning where a model learns the training data **too well**, including its noise and specific patterns.  This results in a model that performs exceptionally well on the training data but performs poorly on new, unseen data (the test data).  Essentially, the model has memorized the training data instead of learning the underlying, generalizable patterns.\n",
      "\n",
      "**Here's a breakdown of the key aspects:**\n",
      "\n",
      "* **What it means:**\n",
      "    * The model has learned the **noise** and **outliers** present in the training data.\n",
      "    * The model is too complex and has too many parameters.\n",
      "    * The model's decision boundaries are overly intricate and tailored to the specific training instances.\n",
      "    * The model fails to generalize to new data because it's specifically adapted to the quirks of the training set.\n",
      "\n",
      "* **How it happens:**\n",
      "    * **Excessive model complexity:** Using a model with too many parameters (e.g., a high-degree polynomial regression, a deep neural network with many layers) relative to the amount of training data.  This allows the model to \"memorize\" the training data.\n",
      "    * **Insufficient training data:**  If the training data isn't representative of the real-world data, the model may learn spurious correlations that don't hold true in general.\n",
      "    * **Over-training:** Training the model for too long, allowing it to gradually fit the noise in the training data.\n",
      "    * **Presence of noise and outliers:**  Noisy data points can mislead the model into learning incorrect patterns.\n",
      "\n",
      "* **Symptoms of Overfitting:**\n",
      "    * **High accuracy on the training data, but low accuracy on the test data.** This is the most common and obvious indicator.\n",
      "    * **Large difference between training and testing performance metrics (e.g., accuracy, F1-score, AUC).**\n",
      "    * **Complex and irregular decision boundaries (for classification tasks).** This can be visualized in lower-dimensional spaces.\n",
      "    * **High variance in model predictions.**  The model's predictions may be very sensitive to small changes in the input data.\n",
      "\n",
      "* **Visual Representation:**\n",
      "\n",
      "   Imagine fitting a curve to a set of data points.\n",
      "\n",
      "   * **Underfitting:** A straight line (simple model) doesn't capture the underlying trend of the data.  High bias, low variance.\n",
      "   * **Just Right (Good Fit):** A smooth curve captures the underlying trend without being overly sensitive to individual data points.  Balance between bias and variance.\n",
      "   * **Overfitting:** A highly complex, wiggly curve passes through almost all the data points, including the noise.  Low bias, high variance.\n",
      "\n",
      "* **Why is it bad?**\n",
      "\n",
      "    * **Poor generalization:** The primary goal of machine learning is to build models that can accurately predict outcomes on new, unseen data. Overfitting defeats this purpose.\n",
      "    * **Unreliable predictions:** The model may produce incorrect or misleading predictions when applied to real-world scenarios.\n",
      "    * **Reduced model usability:**  A model that only performs well on the training data is essentially useless in practice.\n",
      "\n",
      "* **How to Prevent Overfitting:**\n",
      "\n",
      "    * **Increase the amount of training data:**  More data can help the model learn the true underlying patterns and reduce the impact of noise.\n",
      "    * **Simplify the model:** Use a simpler model with fewer parameters (e.g., a lower-degree polynomial, a shallower neural network).\n",
      "    * **Regularization:** Add a penalty term to the loss function that discourages the model from learning overly complex patterns. Common regularization techniques include L1 (Lasso) and L2 (Ridge) regularization.\n",
      "    * **Cross-validation:** Use cross-validation techniques (e.g., k-fold cross-validation) to evaluate the model's performance on multiple subsets of the data and get a more reliable estimate of its generalization ability.\n",
      "    * **Early stopping:** Monitor the model's performance on a validation set during training and stop training when the performance starts to decrease.  This prevents the model from over-training on the training data.\n",
      "    * **Dropout (for neural networks):** Randomly drop out some neurons during training, forcing the network to learn more robust and generalizable features.\n",
      "    * **Data augmentation:**  Create new training examples by applying transformations to the existing data (e.g., rotating, scaling, or cropping images). This increases the size and diversity of the training set.\n",
      "    * **Feature selection:** Choose the most relevant features and discard irrelevant or redundant ones. This can help to simplify the model and reduce the risk of overfitting.\n",
      "\n",
      "**In summary, overfitting is a critical issue in machine learning that can significantly degrade the performance of a model on unseen data. Understanding the causes and symptoms of overfitting, and applying appropriate techniques to prevent it, is essential for building robust and reliable machine learning models.**\n",
      "\n",
      "Relevance Score: 0.70\n",
      "Specificity Score: 0.48\n",
      "\n",
      "Manual Evaluation:\n",
      "Prompt: Explain the concept of overfitting in machine learning.\n",
      "Response: ## Overfitting in Machine Learning: A Detailed Explanation\n",
      "\n",
      "Overfitting is a common problem in machine learning where a model learns the training data **too well**, including its noise and specific patterns.  This results in a model that performs exceptionally well on the training data but performs poorly on new, unseen data (the test data).  Essentially, the model has memorized the training data instead of learning the underlying, generalizable patterns.\n",
      "\n",
      "**Here's a breakdown of the key aspects:**\n",
      "\n",
      "* **What it means:**\n",
      "    * The model has learned the **noise** and **outliers** present in the training data.\n",
      "    * The model is too complex and has too many parameters.\n",
      "    * The model's decision boundaries are overly intricate and tailored to the specific training instances.\n",
      "    * The model fails to generalize to new data because it's specifically adapted to the quirks of the training set.\n",
      "\n",
      "* **How it happens:**\n",
      "    * **Excessive model complexity:** Using a model with too many parameters (e.g., a high-degree polynomial regression, a deep neural network with many layers) relative to the amount of training data.  This allows the model to \"memorize\" the training data.\n",
      "    * **Insufficient training data:**  If the training data isn't representative of the real-world data, the model may learn spurious correlations that don't hold true in general.\n",
      "    * **Over-training:** Training the model for too long, allowing it to gradually fit the noise in the training data.\n",
      "    * **Presence of noise and outliers:**  Noisy data points can mislead the model into learning incorrect patterns.\n",
      "\n",
      "* **Symptoms of Overfitting:**\n",
      "    * **High accuracy on the training data, but low accuracy on the test data.** This is the most common and obvious indicator.\n",
      "    * **Large difference between training and testing performance metrics (e.g., accuracy, F1-score, AUC).**\n",
      "    * **Complex and irregular decision boundaries (for classification tasks).** This can be visualized in lower-dimensional spaces.\n",
      "    * **High variance in model predictions.**  The model's predictions may be very sensitive to small changes in the input data.\n",
      "\n",
      "* **Visual Representation:**\n",
      "\n",
      "   Imagine fitting a curve to a set of data points.\n",
      "\n",
      "   * **Underfitting:** A straight line (simple model) doesn't capture the underlying trend of the data.  High bias, low variance.\n",
      "   * **Just Right (Good Fit):** A smooth curve captures the underlying trend without being overly sensitive to individual data points.  Balance between bias and variance.\n",
      "   * **Overfitting:** A highly complex, wiggly curve passes through almost all the data points, including the noise.  Low bias, high variance.\n",
      "\n",
      "* **Why is it bad?**\n",
      "\n",
      "    * **Poor generalization:** The primary goal of machine learning is to build models that can accurately predict outcomes on new, unseen data. Overfitting defeats this purpose.\n",
      "    * **Unreliable predictions:** The model may produce incorrect or misleading predictions when applied to real-world scenarios.\n",
      "    * **Reduced model usability:**  A model that only performs well on the training data is essentially useless in practice.\n",
      "\n",
      "* **How to Prevent Overfitting:**\n",
      "\n",
      "    * **Increase the amount of training data:**  More data can help the model learn the true underlying patterns and reduce the impact of noise.\n",
      "    * **Simplify the model:** Use a simpler model with fewer parameters (e.g., a lower-degree polynomial, a shallower neural network).\n",
      "    * **Regularization:** Add a penalty term to the loss function that discourages the model from learning overly complex patterns. Common regularization techniques include L1 (Lasso) and L2 (Ridge) regularization.\n",
      "    * **Cross-validation:** Use cross-validation techniques (e.g., k-fold cross-validation) to evaluate the model's performance on multiple subsets of the data and get a more reliable estimate of its generalization ability.\n",
      "    * **Early stopping:** Monitor the model's performance on a validation set during training and stop training when the performance starts to decrease.  This prevents the model from over-training on the training data.\n",
      "    * **Dropout (for neural networks):** Randomly drop out some neurons during training, forcing the network to learn more robust and generalizable features.\n",
      "    * **Data augmentation:**  Create new training examples by applying transformations to the existing data (e.g., rotating, scaling, or cropping images). This increases the size and diversity of the training set.\n",
      "    * **Feature selection:** Choose the most relevant features and discard irrelevant or redundant ones. This can help to simplify the model and reduce the risk of overfitting.\n",
      "\n",
      "**In summary, overfitting is a critical issue in machine learning that can significantly degrade the performance of a model on unseen data. Understanding the causes and symptoms of overfitting, and applying appropriate techniques to prevent it, is essential for building robust and reliable machine learning models.**\n",
      "\n",
      "Evaluation Criteria:\n",
      "Clarity: 10.0/10\n",
      "Accuracy: 10.0/10\n",
      "Relevance: 10.0/10\n",
      "\n",
      "Additional Comments:\n",
      "Comments: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Explain the concept of overfitting in machine learning.',\n",
       " 'response': '## Overfitting in Machine Learning: A Detailed Explanation\\n\\nOverfitting is a common problem in machine learning where a model learns the training data **too well**, including its noise and specific patterns.  This results in a model that performs exceptionally well on the training data but performs poorly on new, unseen data (the test data).  Essentially, the model has memorized the training data instead of learning the underlying, generalizable patterns.\\n\\n**Here\\'s a breakdown of the key aspects:**\\n\\n* **What it means:**\\n    * The model has learned the **noise** and **outliers** present in the training data.\\n    * The model is too complex and has too many parameters.\\n    * The model\\'s decision boundaries are overly intricate and tailored to the specific training instances.\\n    * The model fails to generalize to new data because it\\'s specifically adapted to the quirks of the training set.\\n\\n* **How it happens:**\\n    * **Excessive model complexity:** Using a model with too many parameters (e.g., a high-degree polynomial regression, a deep neural network with many layers) relative to the amount of training data.  This allows the model to \"memorize\" the training data.\\n    * **Insufficient training data:**  If the training data isn\\'t representative of the real-world data, the model may learn spurious correlations that don\\'t hold true in general.\\n    * **Over-training:** Training the model for too long, allowing it to gradually fit the noise in the training data.\\n    * **Presence of noise and outliers:**  Noisy data points can mislead the model into learning incorrect patterns.\\n\\n* **Symptoms of Overfitting:**\\n    * **High accuracy on the training data, but low accuracy on the test data.** This is the most common and obvious indicator.\\n    * **Large difference between training and testing performance metrics (e.g., accuracy, F1-score, AUC).**\\n    * **Complex and irregular decision boundaries (for classification tasks).** This can be visualized in lower-dimensional spaces.\\n    * **High variance in model predictions.**  The model\\'s predictions may be very sensitive to small changes in the input data.\\n\\n* **Visual Representation:**\\n\\n   Imagine fitting a curve to a set of data points.\\n\\n   * **Underfitting:** A straight line (simple model) doesn\\'t capture the underlying trend of the data.  High bias, low variance.\\n   * **Just Right (Good Fit):** A smooth curve captures the underlying trend without being overly sensitive to individual data points.  Balance between bias and variance.\\n   * **Overfitting:** A highly complex, wiggly curve passes through almost all the data points, including the noise.  Low bias, high variance.\\n\\n* **Why is it bad?**\\n\\n    * **Poor generalization:** The primary goal of machine learning is to build models that can accurately predict outcomes on new, unseen data. Overfitting defeats this purpose.\\n    * **Unreliable predictions:** The model may produce incorrect or misleading predictions when applied to real-world scenarios.\\n    * **Reduced model usability:**  A model that only performs well on the training data is essentially useless in practice.\\n\\n* **How to Prevent Overfitting:**\\n\\n    * **Increase the amount of training data:**  More data can help the model learn the true underlying patterns and reduce the impact of noise.\\n    * **Simplify the model:** Use a simpler model with fewer parameters (e.g., a lower-degree polynomial, a shallower neural network).\\n    * **Regularization:** Add a penalty term to the loss function that discourages the model from learning overly complex patterns. Common regularization techniques include L1 (Lasso) and L2 (Ridge) regularization.\\n    * **Cross-validation:** Use cross-validation techniques (e.g., k-fold cross-validation) to evaluate the model\\'s performance on multiple subsets of the data and get a more reliable estimate of its generalization ability.\\n    * **Early stopping:** Monitor the model\\'s performance on a validation set during training and stop training when the performance starts to decrease.  This prevents the model from over-training on the training data.\\n    * **Dropout (for neural networks):** Randomly drop out some neurons during training, forcing the network to learn more robust and generalizable features.\\n    * **Data augmentation:**  Create new training examples by applying transformations to the existing data (e.g., rotating, scaling, or cropping images). This increases the size and diversity of the training set.\\n    * **Feature selection:** Choose the most relevant features and discard irrelevant or redundant ones. This can help to simplify the model and reduce the risk of overfitting.\\n\\n**In summary, overfitting is a critical issue in machine learning that can significantly degrade the performance of a model on unseen data. Understanding the causes and symptoms of overfitting, and applying appropriate techniques to prevent it, is essential for building robust and reliable machine learning models.**',\n",
       " 'relevance': 0.703323452493585,\n",
       " 'specificity': 0.4814305364511692}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_prompt(prompt, expected_content, manual_criteria=['Clarity', 'Accuracy', 'Relevance']):\n",
    "    \"\"\"Perform a comprehensive evaluation of a prompt using both manual and automated techniques.\"\"\"\n",
    "    response = llm.invoke(prompt).content\n",
    "    \n",
    "    print(\"Automated Evaluation:\")\n",
    "    auto_results = automated_evaluation(prompt, response, expected_content)\n",
    "    \n",
    "    print(\"\\nManual Evaluation:\")\n",
    "    manual_evaluation(prompt, response, manual_criteria)\n",
    "    \n",
    "    return {\"prompt\": prompt, \"response\": response, **auto_results}\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain the concept of overfitting in machine learning.\"\n",
    "expected_content = \"Overfitting occurs when a model learns the training data too well, including its noise and fluctuations, leading to poor generalization on new, unseen data.\"\n",
    "evaluate_prompt(prompt, expected_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90f359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
